{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dce4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_no = 0\n",
    "universe_id = \"test\"\n",
    "universe = {\n",
    "    #\"scale\": \"scale\", # \"scale\", \"do-not-scale\",\n",
    "    #\"encode_categorical\": \"one-hot\", # \"ordinal\", \"one-hot\"\n",
    "    \"model\": \"logreg\", # \"logreg\", \"penalized_logreg\", \"rf\",\n",
    "    \"cutoff\": [\"quantile_0.15\", \"quantile_0.30\"],\n",
    "    \"exclude_features\": \"none\", # \"none\", \"nationality\", \"sex\", \"nationality-sex\"\n",
    "    \"exclude_subgroups\": \"keep-all\", # \"keep-all\", \"drop-non-german\"\n",
    "}\n",
    "\n",
    "output_dir=\"./output\"\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1650acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Parse universe into dict if it is passed as a string\n",
    "if isinstance(universe, str):\n",
    "    universe = json.loads(universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16620c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload the custom package\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport fairness_multiverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c5c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.universe import UniverseAnalysis\n",
    "\n",
    "universe_analysis = UniverseAnalysis(\n",
    "    run_no = run_no,\n",
    "    universe_id = universe_id,\n",
    "    universe = universe,\n",
    "    output_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106241f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "parsed_seed = int(seed)\n",
    "np.random.seed(parsed_seed)\n",
    "print(f\"Using Seed: {parsed_seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebdc57",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681925a3",
   "metadata": {},
   "source": [
    "Load siab_train, siab_test, siab_calib and/or \n",
    "load siab_train_features, siab_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0496b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SIAB data from cache: data/siab_cached.csv.gz\n",
      "(643690, 164)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "raw_file = Path(\"data/raw/siab.csv\")\n",
    "cache_file = Path(\"data/siab_cached.csv.gz\")\n",
    "\n",
    "# Ensure cache directory exists\n",
    "cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load with simple caching\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading SIAB data from cache: {cache_file}\")\n",
    "    siab = pd.read_csv(cache_file, compression='gzip')\n",
    "else:\n",
    "    print(f\"Cache not found. Reading raw SIAB data: {raw_file}\")\n",
    "    siab = pd.read_csv(raw_file)\n",
    "    siab.to_csv(cache_file, index=False, compression='gzip')\n",
    "    print(f\"Cached SIAB data to: {cache_file}\")\n",
    "\n",
    "# Now use `siab` DataFrame as needed\n",
    "print(siab.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0edb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./data/X_train.csv\")\n",
    "y_train = pd.read_csv(\"./data/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "siab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49c3ce",
   "metadata": {},
   "source": [
    "Pre-Processing for Selected Task -> skipped. I think I don't need this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a1b33",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca879031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude protected features\n",
    "# \"exclude_features\": \"none\", # \"nationality\", \"sex\", \"nationality-sex\"\n",
    "\n",
    "# ToDo: incorporate maxdeutsch.Missing into maxdeutsch1\n",
    "\n",
    "excluded_features = universe[\"exclude_features\"].split(\"-\") # split, e.g.: \"nationality-sex\" -> [\"nationality\", \"sex\"]\n",
    "excluded_features_dictionary = {\n",
    "    \"nationality\": [\"maxdeutsch1\", \"maxdeutsch.Missing.\"],\n",
    "    \"sex\": [\"frau1\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_once(mixed_list):\n",
    "    flat = []\n",
    "    for item in mixed_list:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            flat.extend(item)\n",
    "        else:\n",
    "            flat.append(item)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b745ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code nice names to column names\n",
    "# ???? empty\n",
    "\n",
    "excluded_features_columns = [\n",
    "    excluded_features_dictionary[f] for f in excluded_features if len(f) > 0 and f != \"none\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f84f73a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "from utils import flatten_once\n",
    "\n",
    "excluded_features_columns = flatten_once(excluded_features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "884dea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(excluded_features_columns) > 0:\n",
    "    print(f\"Dropping features: {excluded_features_columns}\")\n",
    "    X_train.drop(excluded_features_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude Certain Subgroups\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
