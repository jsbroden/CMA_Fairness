{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dce4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_no = 0\n",
    "universe_id = \"test\"\n",
    "universe = {\n",
    "    #\"scale\": \"scale\", # \"scale\", \"do-not-scale\",\n",
    "    #\"encode_categorical\": \"one-hot\", # \"ordinal\", \"one-hot\"\n",
    "    \"model\": \"logreg\", # \"logreg\", \"penalized_logreg\", \"rf\",\n",
    "    \"cutoff\": [\"quantile_0.15\", \"quantile_0.30\"],\n",
    "    \"exclude_features\": \"nationality\", # \"none\", \"nationality\", \"sex\", \"nationality-sex\"\n",
    "    \"exclude_subgroups\": \"drop-non-german\", # \"keep-all\", \"drop-non-german\"\n",
    "}\n",
    "\n",
    "output_dir=\"./output\"\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1650acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Parse universe into dict if it is passed as a string\n",
    "if isinstance(universe, str):\n",
    "    universe = json.loads(universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16620c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto-reload the custom package\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport fairness_multiverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01c5c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.universe import UniverseAnalysis\n",
    "\n",
    "universe_analysis = UniverseAnalysis(\n",
    "    run_no = run_no,\n",
    "    universe_id = universe_id,\n",
    "    universe = universe,\n",
    "    output_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "106241f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "parsed_seed = int(seed)\n",
    "np.random.seed(parsed_seed)\n",
    "print(f\"Using Seed: {parsed_seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebdc57",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681925a3",
   "metadata": {},
   "source": [
    "Load siab_train, siab_test, siab_calib and/or \n",
    "load siab_train_features, siab_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0496b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SIAB data from cache: data/siab_cached.csv.gz\n",
      "(643690, 164)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "raw_file = Path(\"data/raw/siab.csv\")\n",
    "cache_file = Path(\"data/siab_cached.csv.gz\")\n",
    "\n",
    "# Ensure cache directory exists\n",
    "cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load with simple caching\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading SIAB data from cache: {cache_file}\")\n",
    "    siab = pd.read_csv(cache_file, compression='gzip')\n",
    "else:\n",
    "    print(f\"Cache not found. Reading raw SIAB data: {raw_file}\")\n",
    "    siab = pd.read_csv(raw_file)\n",
    "    siab.to_csv(cache_file, index=False, compression='gzip')\n",
    "    print(f\"Cached SIAB data to: {cache_file}\")\n",
    "\n",
    "# Now use `siab` DataFrame as needed\n",
    "print(siab.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0edb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./data/X_train.csv\")\n",
    "y_train = pd.read_csv(\"./data/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "siab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49c3ce",
   "metadata": {},
   "source": [
    "Pre-Processing for Selected Task -> skipped. I think I don't need this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a1b33",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca879031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude protected features\n",
    "# \"exclude_features\": \"none\", # \"nationality\", \"sex\", \"nationality-sex\"\n",
    "\n",
    "# ToDo: incorporate maxdeutsch.Missing into maxdeutsch1\n",
    "\n",
    "excluded_features = universe[\"exclude_features\"].split(\"-\") # split, e.g.: \"nationality-sex\" -> [\"nationality\", \"sex\"]\n",
    "excluded_features_dictionary = {\n",
    "    \"nationality\": [\"maxdeutsch1\", \"maxdeutsch.Missing.\"],\n",
    "    \"sex\": [\"frau1\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b745ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code nice names to column names\n",
    "# ???? empty\n",
    "\n",
    "excluded_features_columns = [\n",
    "    excluded_features_dictionary[f] for f in excluded_features if len(f) > 0 and f != \"none\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f84f73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import flatten_once\n",
    "\n",
    "excluded_features_columns = flatten_once(excluded_features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "884dea22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping features: ['maxdeutsch1', 'maxdeutsch.Missing.']\n"
     ]
    }
   ],
   "source": [
    "if len(excluded_features_columns) > 0:\n",
    "    print(f\"Dropping features: {excluded_features_columns}\")\n",
    "    X_train.drop(excluded_features_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc5f2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude Certain Subgroups\n",
    "# ??? just training or also calibration? \n",
    "\n",
    "# Extract configuration\n",
    "exclude_subgroups_config = universe[\"exclude_subgroups\"].split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dea6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(exclude_subgroups_config) == 1:\n",
    "    exclude_subgroups_config = (exclude_subgroups_config[0], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc563af",
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_subgroups_method, excl_subgroup_colname, excl_subgroups_value = exclude_subgroups_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3f3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if excl_subgroup_colname == \"nationality\":\n",
    "    excl_subgroup_column = [\"maxdeutsch1\", \"maxdeutsch.Missing.\"]\n",
    "    excl_subgroup_counts = org_train[excl_subgroup_column].value_counts()\n",
    "elif excl_subgroups_method != \"keep-all\":\n",
    "    raise Exception(\"Unsupported configuration for exclude_subgroups:\" + universe[\"exclude_subgroups\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d08c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if excl_subgroups_method == \"keep-all\":\n",
    "    # Don't need to do anything\n",
    "    excl_subgroup_column = None\n",
    "    excl_subgroup_values = []\n",
    "else:\n",
    "    if excl_subgroups_method == \"drop-smallest\":\n",
    "        drop_smallest_n = int(excl_subgroups_value)\n",
    "        excl_subgroup_values = list(excl_subgroup_counts.tail(drop_smallest_n).index)\n",
    "    elif excl_subgroups_method == \"keep-largest\":\n",
    "        keep_largest_n = int(excl_subgroups_value)\n",
    "        excl_subgroup_values = list(excl_subgroup_counts.tail(\n",
    "            len(excl_subgroup_counts) - keep_largest_n\n",
    "        ).index)\n",
    "    elif excl_subgroups_method == \"drop-name\":\n",
    "        excl_subgroup_values = [excl_subgroups_value]\n",
    "    else:\n",
    "        raise Exception(\"Unsupported configuration for exclude_subgroups:\" + universe[\"exclude_subgroups\"])\n",
    "\n",
    "    if excl_subgroup_column is not None:\n",
    "        print(f\"Dropping values: {excl_subgroup_values}\")\n",
    "        keep_rows_mask = ~org_train[excl_subgroup_column].isin(excl_subgroup_values)\n",
    "\n",
    "    n_rows_to_drop = (~keep_rows_mask).sum()\n",
    "    if n_rows_to_drop > 0:\n",
    "        print(f\"Dropping N = {n_rows_to_drop} ({n_rows_to_drop / len(keep_rows_mask):.2%}) rows from {excl_subgroup_colname}\")\n",
    "        X_train = X_train[keep_rows_mask]\n",
    "        y_train = y_train[keep_rows_mask]\n",
    "        group_train = group_train[keep_rows_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt\n",
    "\n",
    "subgrp_method = universe[\"exclude_subgroups\"]\n",
    "\n",
    "# If we're not keeping all, define a mask to drop any non-German nationals\n",
    "if subgrp_method == \"keep-all\":\n",
    "    # No changes\n",
    "    X_train_filtered = X_train.copy()\n",
    "    y_train_filtered = y_train.copy()\n",
    "    group_train_filtered = group_train.copy()\n",
    "\n",
    "elif subgrp_method == \"drop-non-german\":\n",
    "    # Assume 'nationality' is encoded in column \"maxdeutsch1\" with value 1 == German\n",
    "    mask_is_german = (org_train[\"maxdeutsch1\"] == 1)\n",
    "    n_drop = (~mask_is_german).sum()\n",
    "    pct_drop = n_drop / len(mask_is_german)\n",
    "    print(f\"Dropping N = {n_drop} ({pct_drop:.2%}) non-German rows\")\n",
    "\n",
    "    X_train_filtered = X_train[mask_is_german]\n",
    "    y_train_filtered = y_train[mask_is_german]\n",
    "    group_train_filtered = group_train[mask_is_german]\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported configuration for exclude_subgroups: {subgrp_method}\")\n",
    "\n",
    "# Now reassign (or continue working with) the filtered sets:\n",
    "X_train, y_train, group_train = X_train_filtered, y_train_filtered, group_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a7b43cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'maxdeutsch1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'maxdeutsch1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m     keep_mask = pd.Series(\u001b[38;5;28;01mTrue\u001b[39;00m, index=X_train.index)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m universe[\u001b[33m\"\u001b[39m\u001b[33mexclude_subgroups\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mdrop-non-german\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# keep only rows where maxdeutsch1 == 1 (German nationals)\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# this will automatically drop both non-German (0) and Missing (NaN or 0+Missing flag)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     keep_mask = \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxdeutsch1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m == \u001b[32m1\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# optional: if maxdeutsch1==0 but maxdeutsch.Missing.==1,\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# you might want to treat those separately (e.g. impute or tag).\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# But for “drop-non-german” we just drop everything not ==1.\u001b[39;00m\n\u001b[32m     16\u001b[39m     n_drop = (~keep_mask).sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'maxdeutsch1'"
     ]
    }
   ],
   "source": [
    "# gpt2\n",
    "# Would that still work if protected features are excluded and non-germans should be dropped?\n",
    "\n",
    "if universe[\"exclude_subgroups\"] == \"keep-all\":\n",
    "    keep_mask = pd.Series(True, index=X_train.index)\n",
    "\n",
    "elif universe[\"exclude_subgroups\"] == \"drop-non-german\":\n",
    "    # keep only rows where maxdeutsch1 == 1 (German nationals)\n",
    "    # this will automatically drop both non-German (0) and Missing (NaN or 0+Missing flag)\n",
    "    keep_mask = X_train[\"maxdeutsch1\"] == 1\n",
    "\n",
    "    # optional: if maxdeutsch1==0 but maxdeutsch.Missing.==1,\n",
    "    # you might want to treat those separately (e.g. impute or tag).\n",
    "    # But for “drop-non-german” we just drop everything not ==1.\n",
    "    \n",
    "    n_drop = (~keep_mask).sum()\n",
    "    print(f\"Dropping N = {n_drop} ({n_drop/len(keep_mask):.2%}) non-German or missing rows\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported exclude_subgroups: {universe['exclude_subgroups']}\")\n",
    "\n",
    "# --- 3) Apply the mask ---\n",
    "X_train   = X_train[keep_mask]\n",
    "y_train   = y_train[keep_mask]\n",
    "#group_train = group_train[keep_mask]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
