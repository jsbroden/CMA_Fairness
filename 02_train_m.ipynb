{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7bebcf4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08327b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate # GroupKFold, GridSearchCV,\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from joblib import dump\n",
    "\n",
    "from utils import (\n",
    "    precision_at_k,\n",
    "    recall_at_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9275fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load universe definitions from YAML\n",
    "import yaml\n",
    "\n",
    "with open(\"universes.yaml\") as f:\n",
    "    universes = yaml.safe_load(f)\n",
    "\n",
    "# Access by ID\n",
    "#universe_id = 12\n",
    "#config = next(u for u in universes if u[\"id\"] == universe_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298f2b2",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa524e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f = pd.read_csv(\"./output/X_train_f.csv\") # 2010 - 2014, w. protected attributes\n",
    "X_train_s = pd.read_csv(\"./output/X_train_s.csv\") # 2010 - 2014, w/o protected attributes\n",
    "y_train = pd.read_csv(\"./output/y_train.csv\").iloc[:,0]\n",
    "\n",
    "X_test_f = pd.read_csv(\"./output/X_test_f.csv\")\n",
    "X_test_s = pd.read_csv(\"./output/X_test_s.csv\")\n",
    "y_test = pd.read_csv(\"./output/y_test.csv\").iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d403690f",
   "metadata": {},
   "source": [
    "## Multiverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28fde7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_type, X, y):\n",
    "    if model_type == \"logreg\":\n",
    "        model = LogisticRegression(penalty=None, solver=\"newton-cg\", max_iter=1000)\n",
    "    elif model_type == \"penalized_logreg\":\n",
    "        model = LogisticRegression(penalty=\"l2\", C=1.0, solver=\"newton-cg\", max_iter=1000)\n",
    "    elif model_type == \"rf\":\n",
    "        model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83f5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test feature sets\n",
    "feature_sets_train = {\n",
    "    \"with_protected\": X_train_f,\n",
    "    \"without_protected\": X_train_s\n",
    "}\n",
    "feature_sets_test = {\n",
    "    \"with_protected\": X_test_f,\n",
    "    \"without_protected\": X_test_s\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0123b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds\n",
    "threshold_policies = {\n",
    "    \"top15\": 0.15,\n",
    "    \"top30\": 0.30\n",
    "}\n",
    "\n",
    "#    \"top45\": 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84fca6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group universes by (model_type, feature_flag)\n",
    "from collections import defaultdict\n",
    "universe_groups = defaultdict(list)\n",
    "for cfg in universes:\n",
    "    key = (cfg[\"model\"], cfg[\"feature_set\"])\n",
    "    universe_groups[key].append(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17513cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: logreg with with_protected features\n",
      "Predicted universe 1: logreg, with_protected, top15\n",
      "Predicted universe 2: logreg, with_protected, top30\n",
      "Training model: logreg with without_protected features\n",
      "Predicted universe 3: logreg, without_protected, top15\n",
      "Predicted universe 4: logreg, without_protected, top30\n",
      "Training model: penalized_logreg with with_protected features\n",
      "Predicted universe 5: penalized_logreg, with_protected, top15\n",
      "Predicted universe 6: penalized_logreg, with_protected, top30\n",
      "Training model: penalized_logreg with without_protected features\n",
      "Predicted universe 7: penalized_logreg, without_protected, top15\n",
      "Predicted universe 8: penalized_logreg, without_protected, top30\n",
      "Training model: rf with with_protected features\n",
      "Predicted universe 9: rf, with_protected, top15\n",
      "Predicted universe 10: rf, with_protected, top30\n",
      "Training model: rf with without_protected features\n",
      "Predicted universe 11: rf, without_protected, top15\n",
      "Predicted universe 12: rf, without_protected, top30\n"
     ]
    }
   ],
   "source": [
    "# Train one model per (model_type, feature_flag), then apply all thresholds\n",
    "predictions_by_universe = {}\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "for (model_type, feature_flag), cfgs in universe_groups.items():\n",
    "    print(f\"Training model: {model_type} with {feature_flag} features\")\n",
    "    X_train_used = feature_sets_train[feature_flag]\n",
    "    model = train_model(model_type, X_train_used, y_train)\n",
    "\n",
    "    # Save model\n",
    "    universe_id = f\"{model_type}_{feature_flag}\"\n",
    "    dump(model, f\"./models/{universe_id}.joblib\")\n",
    "\n",
    "    # Predict probabilities on test set\n",
    "    X_test_used = feature_sets_test[feature_flag]\n",
    "    probs = model.predict_proba(X_test_used)[:, 1]\n",
    "\n",
    "    for cfg in cfgs:\n",
    "        uid = cfg[\"id\"]\n",
    "        threshold_key = cfg[\"threshold_policy\"]\n",
    "        k = threshold_policies[threshold_key]\n",
    "        threshold_value = np.sort(probs)[::-1][int(k * len(probs))]\n",
    "        binary_preds = (probs >= threshold_value).astype(int)\n",
    "        predictions_by_universe[uid] = binary_preds\n",
    "        print(f\"Predicted universe {uid}: {model_type}, {feature_flag}, {threshold_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315510f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined predictions to ./output/preds_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine results into a DataFrame for inspection/saving\n",
    "y_test_array = np.array(y_test).reshape(-1, 1)\n",
    "y_df = pd.DataFrame(y_test_array, columns=[\"y_test\"])\n",
    "\n",
    "all_preds = []\n",
    "for uid in sorted(predictions_by_universe):\n",
    "    col_name = f\"preds_u{uid}\"\n",
    "    col_data = pd.DataFrame(predictions_by_universe[uid], columns=[col_name])\n",
    "    all_preds.append(col_data)\n",
    "\n",
    "preds_test = pd.concat([y_df] + all_preds, axis=1)\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "preds_test.to_csv(\"./output/preds_test.csv\", index=False)\n",
    "\n",
    "print(\"Saved combined predictions to ./output/preds_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5ed6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h7/6qcvyjh51cg86vrxn3xs8c_40000gn/T/ipykernel_33455/2427680122.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  universe_df_escaped = universe_df.applymap(escape_latex_str)\n"
     ]
    }
   ],
   "source": [
    "# Create LaTeX summary table for universes\n",
    "def escape_latex_str(val):\n",
    "    return str(val).replace('_', '\\\\_')\n",
    "\n",
    "universe_df = pd.DataFrame(universes)\n",
    "\n",
    "# Rename and reorder columns\n",
    "universe_df = universe_df.rename(columns={\n",
    "    \"feature_set\": \"feature set\",\n",
    "    \"threshold_policy\": \"threshold\"\n",
    "})\n",
    "universe_df = universe_df[[\"id\", \"model\", \"feature set\", \"threshold\"]]\n",
    "\n",
    "universe_df_escaped = universe_df.applymap(escape_latex_str)\n",
    "\n",
    "latex_table = universe_df_escaped.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Universe configuration overview\",\n",
    "    label=\"tab:universe_summary\",\n",
    "    escape=False\n",
    ")\n",
    "\n",
    "with open(\"./output/universe_summary.tex\", \"w\") as f:\n",
    "    f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61b3ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LaTeX summary table to ./output/universe_summary.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h7/6qcvyjh51cg86vrxn3xs8c_40000gn/T/ipykernel_33455/4211774802.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  universe_df_escaped = universe_df.applymap(escape_latex_str)\n"
     ]
    }
   ],
   "source": [
    "# Escape LaTeX special characters in universe config\n",
    "def escape_latex_str(val):\n",
    "    return str(val).replace('_', '\\\\_')\n",
    "\n",
    "universe_df = pd.DataFrame(universes)\n",
    "universe_df_escaped = universe_df.applymap(escape_latex_str)\n",
    "\n",
    "latex_table = universe_df_escaped.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Universe configuration overview\",\n",
    "    label=\"tab:universe_summary\",\n",
    "    escape=False\n",
    ")\n",
    "\n",
    "with open(\"./output/universe_summary.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"Saved LaTeX summary table to ./output/universe_summary.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6157e",
   "metadata": {},
   "source": [
    "## 01 Logit Regression (w. protected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm1 = LogisticRegression(penalty = None, solver = 'newton-cg', max_iter = 1000)\n",
    "glm1.fit(X_train_f, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs1 = pd.DataFrame(X_train_f.columns, columns = ['var'])\n",
    "coefs1['coef'] = pd.DataFrame(glm1.coef_).transpose()\n",
    "\n",
    "# Build a DataFrame of feature names + their learned coefficients, to inspect which variables \n",
    "# (including protected attrs) the model weights most heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef186a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(glm1, './models/glm1.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9880d",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "glmcv1 = cross_validate(estimator = glm1, \n",
    "                       X = X_train_f,\n",
    "                       y = y_train,\n",
    "                       cv = tscv,\n",
    "                       n_jobs = -1, # use all available cores\n",
    "                       scoring = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV output\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(glmcv1)\n",
    "\n",
    "# Only keep test scores\n",
    "test_scores = results_df.filter(like='test_')\n",
    "\n",
    "# Summary statistics\n",
    "summary = test_scores.agg(['mean', 'std']).T\n",
    "summary.columns = ['mean', 'std']\n",
    "print(summary)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bar chart of mean ± std\n",
    "summary.plot(kind='bar', y='mean', yerr='std', legend=False, capsize=4)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Cross-Validation Performance (mean ± std)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9538551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV output\n",
    "\n",
    "test_scores.T.plot(marker='o')\n",
    "plt.title(\"Cross-Validation Scores per Fold\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a68bde",
   "metadata": {},
   "source": [
    "## 02 Logit Regression (w/o protected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm2 = LogisticRegression(penalty = None, solver = 'newton-cg', max_iter = 1000)\n",
    "glm2.fit(X_train_s, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs2 = pd.DataFrame(X_train_s.columns, columns = ['var'])\n",
    "coefs2['coef'] = pd.DataFrame(glm2.coef_).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(glm2, './models/glm2.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9509612",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "k45 = 0.55 # Top 55% \n",
    "k30 = 0.30 # Top 30% \n",
    "k15 = 0.15 # Top 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm1_p = glm1.predict_proba(X_test_f)[:,1] # glm1\n",
    "\n",
    "# Generate the predicted probability of the positive class for each test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da52b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold45 = np.sort(glm1_p)[::-1][int(k45*len(glm1_p))]\n",
    "threshold30 = np.sort(glm1_p)[::-1][int(k30*len(glm1_p))]\n",
    "threshold15 = np.sort(glm1_p)[::-1][int(k15*len(glm1_p))] # threshold15 is the score above which only the top 15% of test samples lie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e5088",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm1_c1 = glm1_p.copy()\n",
    "glm1_c1[glm1_c1 < threshold15] = 0\n",
    "glm1_c1[glm1_c1 >= threshold15] = 1\n",
    "\n",
    "# Create a binary classification vector where only the top 15% by predicted probability are labeled “1”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c693db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm1_c2 = glm1_p.copy()\n",
    "glm1_c2[glm1_c2 < threshold30] = 0\n",
    "glm1_c2[glm1_c2 >= threshold30] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ae60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm1_c3 = glm1_p.copy()\n",
    "glm1_c3[(glm1_c3 <= threshold30) | (glm1_c3 >= threshold15)] = 0\n",
    "glm1_c3[(glm1_c3 > threshold30) & (glm1_c3 < threshold15)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm1_c4 = glm1_p.copy()\n",
    "glm1_c4[(glm1_c4 <= threshold45) | (glm1_c4 >= threshold15)] = 0\n",
    "glm1_c4[(glm1_c4 > threshold45) & (glm1_c4 < threshold15)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a265d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm2_p = glm2.predict_proba(X_test_s)[:,1] # glm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold45 = np.sort(glm2_p)[::-1][int(k45*len(glm2_p))]\n",
    "threshold30 = np.sort(glm2_p)[::-1][int(k30*len(glm2_p))]\n",
    "threshold15 = np.sort(glm2_p)[::-1][int(k15*len(glm2_p))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm2_c1 = glm2_p.copy()\n",
    "glm2_c1[glm2_c1 < threshold15] = 0\n",
    "glm2_c1[glm2_c1 >= threshold15] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18663c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm2_c2 = glm2_p.copy()\n",
    "glm2_c2[glm2_c2 < threshold30] = 0\n",
    "glm2_c2[glm2_c2 >= threshold30] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm2_c3 = glm2_p.copy()\n",
    "glm2_c3[(glm2_c3 <= threshold30) | (glm2_c3 >= threshold15)] = 0\n",
    "glm2_c3[(glm2_c3 > threshold30) & (glm2_c3 < threshold15)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glm2_c4 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d561c",
   "metadata": {},
   "source": [
    "## Performance evaluation -> delete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c80d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, label in zip(\n",
    "    [glm1_c1, glm1_c2, glm1_c3, glm1_c4],\n",
    "    [\"Top 15%\", \"Top 30%\", \"Middle 15-30%\", \"Middle 15-45%\"]\n",
    "):\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1  = f1_score(y_test, preds)\n",
    "    print(f\"{label:15s} → Accuracy: {acc:.3f},  F1-score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e105f",
   "metadata": {},
   "source": [
    "## Combine and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build a single DataFrame side by side with:\n",
    "      - The true labels (‘y_test’)\n",
    "      - The raw predicted probabilities (‘glm1_p’)\n",
    "      - Each binary decision vector at different cutoffs (‘glm1_c1’, ‘glm1_c2’, ‘glm1_c3’).\n",
    "'''\n",
    "\n",
    "preds_test = pd.concat([pd.DataFrame(np.array(y_test), columns = ['y_test']),\n",
    "                         pd.DataFrame(glm1_p, columns = ['glm1_p']),\n",
    "                         pd.DataFrame(glm1_c1, columns = ['glm1_c1']),\n",
    "                         pd.DataFrame(glm1_c2, columns = ['glm1_c2']),\n",
    "                         pd.DataFrame(glm1_c3, columns = ['glm1_c3']),\n",
    "                         pd.DataFrame(glm1_c4, columns = ['glm1_c4']),\n",
    "                         pd.DataFrame(glm2_p, columns = ['glm2_p']),\n",
    "                         pd.DataFrame(glm2_c1, columns = ['glm2_c1']),\n",
    "                         pd.DataFrame(glm2_c2, columns = ['glm2_c2']),\n",
    "                         pd.DataFrame(glm2_c3, columns = ['glm2_c3'])],\n",
    "                        axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caba314",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test.to_csv('./output/preds_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac19281",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577241a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glm1 \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# Define your predictions\n",
    "threshold_preds = {\n",
    "    \"Top 15% (glm1_c1)\": glm1_c1,\n",
    "    \"Top 30% (glm1_c2)\": glm1_c2,\n",
    "    \"Between 15% and 30% (glm1_c3)\": glm1_c3,\n",
    "    \"Between 15% and 45% (glm1_c4)\": glm1_c4,\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "results = []\n",
    "\n",
    "for label, y_pred in threshold_preds.items():\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"Policy\": label,\n",
    "        \"TP\": tp,\n",
    "        \"FP\": fp,\n",
    "        \"TN\": tn,\n",
    "        \"FN\": fn,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df_threshold_metrics = pd.DataFrame(results)\n",
    "print(df_threshold_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glm2 \n",
    "\n",
    "# Define your predictions\n",
    "threshold_preds = {\n",
    "    \"Top 15% (glm2_c1)\": glm2_c1,\n",
    "    \"Top 30% (glm2_c2)\": glm2_c2,\n",
    "    \"Between 15% and 30% (glm1_c3)\": glm2_c3,\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "results = []\n",
    "\n",
    "for label, y_pred in threshold_preds.items():\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"Policy\": label,\n",
    "        \"TP\": tp,\n",
    "        \"FP\": fp,\n",
    "        \"TN\": tn,\n",
    "        \"FN\": fn,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df_threshold_metrics = pd.DataFrame(results)\n",
    "print(df_threshold_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
