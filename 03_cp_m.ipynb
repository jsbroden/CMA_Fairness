{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07550eba",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad59357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "import yaml\n",
    "\n",
    "from utils import (\n",
    "    compute_nc_scores,\n",
    "    find_threshold,\n",
    "    predict_conformal_sets,\n",
    "    evaluate_sets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394b1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load universe definitions from YAML\n",
    "with open(\"universes.yaml\") as f:\n",
    "    universes = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7871a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine unique model & feature_set combinations (each requires one model)\n",
    "unique_combos = {(cfg[\"model\"], cfg[\"feature_set\"]) for cfg in universes}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16090e24",
   "metadata": {},
   "source": [
    "## Data and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8eccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_f = pd.read_csv(\"./output/X_calib_f.csv\") # 2015, w. protected attributes\n",
    "X_calib_s = pd.read_csv(\"./output/X_calib_s.csv\") # 2015, w/o protected attributes\n",
    "y_calib = pd.read_csv(\"./output/y_calib.csv\").iloc[:,0]\n",
    "\n",
    "X_test_f = pd.read_csv(\"./output/X_test_f.csv\")\n",
    "X_test_s = pd.read_csv(\"./output/X_test_s.csv\")\n",
    "y_test = pd.read_csv(\"./output/y_test.csv\").iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ddb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from feature_set name to actual DataFrame, for convenience\n",
    "feature_sets_calib = {\n",
    "    \"with_protected\": X_calib_f,\n",
    "    \"without_protected\": X_calib_s\n",
    "}\n",
    "feature_sets_test = {\n",
    "    \"with_protected\": X_test_f,\n",
    "    \"without_protected\": X_test_s\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbfe25",
   "metadata": {},
   "source": [
    "## Conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34ec667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscoverage level for conformal prediction (10% allowed error rate => 90% target coverage)\n",
    "alpha = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac23896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: given a trained model file and data, produce conformal prediction sets and metrics\n",
    "def conformal_calibrate_and_evaluate(model_path, X_cal, y_cal, X_te, y_te, alpha):\n",
    "    \"\"\"Load model, compute conformal prediction sets on X_te using calibration set (X_cal, y_cal).\"\"\"\n",
    "    model = load(model_path)\n",
    "    # Compute nonconformity scores on calibration set (1 - probability of true class)\n",
    "    probs_cal = model.predict_proba(X_cal)\n",
    "    nc_scores = compute_nc_scores(probs_cal, y_cal)\n",
    "    # Find conformal threshold q_hat for the given alpha (split conformal method)\n",
    "    q_hat = find_threshold(nc_scores, alpha)\n",
    "    # Generate prediction sets for each test example\n",
    "    pred_sets = predict_conformal_sets(model, X_te, q_hat)\n",
    "    # Evaluate coverage and average set size on test data\n",
    "    metrics = evaluate_sets(pred_sets, y_te)\n",
    "    return q_hat, metrics, pred_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e0448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logreg (with_protected - Coverage: 0.911, Avg. Set Size: 1.13\n",
      "Model: logreg (without_protected - Coverage: 0.910, Avg. Set Size: 1.13\n",
      "Model: penalized_logreg (with_protected - Coverage: 0.911, Avg. Set Size: 1.13\n",
      "Model: penalized_logreg (without_protected - Coverage: 0.910, Avg. Set Size: 1.12\n",
      "Model: rf (with_protected - Coverage: 0.914, Avg. Set Size: 1.14\n",
      "Model: rf (without_protected - Coverage: 0.913, Avg. Set Size: 1.14\n"
     ]
    }
   ],
   "source": [
    "# Run conformal prediction for each model type & feature set combination\n",
    "conformal_results = {}  # to collect results for each universe combo\n",
    "for model_type, feature_flag in sorted(unique_combos):\n",
    "    # Identify the saved model file for this universe (matching training stage naming convention)\n",
    "    model_filename = f\"{model_type}_{feature_flag}.joblib\"\n",
    "    model_path = f\"./models/{model_filename}\"\n",
    "    # Select the corresponding calibration and test feature sets\n",
    "    X_cal = feature_sets_calib[feature_flag]\n",
    "    X_te  = feature_sets_test[feature_flag]\n",
    "    # Perform conformal calibration and evaluation\n",
    "    q_hat, metrics, pred_sets = conformal_calibrate_and_evaluate(model_path, X_cal, y_calib, X_te, y_test, alpha)\n",
    "    # Store results (coverage and average set size)\n",
    "    conformal_results[(model_type, feature_flag)] = {\n",
    "        \"q_hat\": q_hat,\n",
    "        \"coverage\": metrics[\"coverage\"],\n",
    "        \"avg_set_size\": metrics[\"avg_size\"]\n",
    "    }\n",
    "    # Print a summary of results for this model universe\n",
    "    cov = metrics[\"coverage\"]; avg = metrics[\"avg_size\"]\n",
    "    print(f\"Model: {model_type} ({feature_flag} - Coverage: {cov:.3f}, Avg. Set Size: {avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404cefef",
   "metadata": {},
   "source": [
    "## Analyzing CP per group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6b9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d4a8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cp_groups(cp_groups):\n",
    "    \"\"\"Compute accuracy, precision, recall, f1 per subgroup in cp_groups DataFrame.\"\"\"\n",
    "\n",
    "    # Evaluate pred_set if needed\n",
    "    if isinstance(cp_groups['pred_set'].iloc[0], str):\n",
    "        cp_groups['pred_set'] = cp_groups['pred_set'].apply(eval)\n",
    "\n",
    "    # Define subgroups\n",
    "    group_definitions = {\n",
    "        'female': lambda df: df['frau1'] == 1,\n",
    "        'male': lambda df: df['frau1'] == 0,\n",
    "        'nongerman': lambda df: df['nongerman'] == 1,\n",
    "        'german': lambda df: df['nongerman'] == 0,\n",
    "        'nongerman_male': lambda df: df['nongerman_male'] == 1,\n",
    "        'nongerman_female': lambda df: df['nongerman_female'] == 1\n",
    "    }\n",
    "\n",
    "    # Compute metrics per group\n",
    "    results = {}\n",
    "    for group, condition in group_definitions.items():\n",
    "        mask = condition(cp_groups) & cp_groups['pred_set'].apply(lambda s: len(s) == 1)\n",
    "        if not np.any(mask):\n",
    "            results[group] = None\n",
    "            continue\n",
    "        y_true = cp_groups.loc[mask, 'true_label']\n",
    "        y_pred = cp_groups.loc[mask, 'pred_set'].apply(lambda s: list(s)[0])\n",
    "        results[group] = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred),\n",
    "            'recall': recall_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393af7b9",
   "metadata": {},
   "source": [
    "## Version 2 ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602ec344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "549cb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformal_prediction_sets(model, X_calib, y_calib, X_test, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Given a trained model and data, produce conformal prediction sets for X_test.\n",
    "    Uses X_calib and y_calib to compute nonconformity scores and threshold at level alpha.\n",
    "    Returns a list of prediction sets (as Python sets) for each row in X_test.\n",
    "    \"\"\"\n",
    "    # Compute nonconformity scores on calibration set: 1 - P_true_label:contentReference[oaicite:11]{index=11}\n",
    "    probs_calib = model.predict_proba(X_calib)  # shape (n_calib, n_classes)\n",
    "    nc_scores = 1.0 - probs_calib[np.arange(len(y_calib)), y_calib]  # compute 1 - P(correct class)\n",
    "    # Determine quantile threshold for given alpha:contentReference[oaicite:12]{index=12}\n",
    "    q_hat = np.quantile(nc_scores, 1 - alpha, method=\"higher\")\n",
    "    # Generate conformal prediction sets for each test sample:contentReference[oaicite:13]{index=13}\n",
    "    probs_test = model.predict_proba(X_test)\n",
    "    nonconf_test = 1.0 - probs_test  # nonconformity for each class on test\n",
    "    pred_sets = [set(np.where(nc_row <= q_hat)[0]) for nc_row in nonconf_test]\n",
    "    return pred_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "052db886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cp_groups(pred_sets, y_test, test_idx, X_test_full):\n",
    "    \"\"\"\n",
    "    Construct a cp_groups DataFrame given conformal prediction sets, true labels, \n",
    "    and test indices. Merges in protected attributes from the full test set.\n",
    "    \"\"\"\n",
    "    # Initialize DataFrame with test indices to align with X_test_full:contentReference[oaicite:14]{index=14}\n",
    "    cp_df = pd.DataFrame(index=test_idx.copy())\n",
    "    cp_df['pred_set']   = pd.Series(pred_sets, index=test_idx).apply(lambda s: {int(x) for x in s})\n",
    "    cp_df['true_label'] = y_test.reindex(test_idx)\n",
    "    # Merge protected attributes from the full test set:contentReference[oaicite:15]{index=15}\n",
    "    cp_df['frau1']   = X_test_full.loc[test_idx, 'frau1']\n",
    "    cp_df['nongerman'] = np.where(\n",
    "        X_test_full.loc[test_idx, 'maxdeutsch1'] == 0, \n",
    "        1, \n",
    "        0\n",
    "    )\n",
    "    # Set 'nongerman' to NaN where nationality is missing:contentReference[oaicite:16]{index=16}\n",
    "    if 'maxdeutsch.Missing.' in X_test_full.columns:\n",
    "        missing_mask = (X_test_full.loc[test_idx, 'maxdeutsch.Missing.'] == 1)\n",
    "        cp_df.loc[missing_mask, 'nongerman'] = np.nan\n",
    "    # Derived subgroup flags:contentReference[oaicite:17]{index=17}\n",
    "    cp_df['nongerman_male']   = np.where((cp_df['nongerman'] == 1) & (cp_df['frau1'] == 0), 1, 0)\n",
    "    cp_df['nongerman_female'] = np.where((cp_df['nongerman'] == 1) & (cp_df['frau1'] == 1), 1, 0)\n",
    "    # Drop any rows with NaN in subgroup columns (e.g., missing protected info):contentReference[oaicite:18]{index=18}\n",
    "    cp_df = cp_df.dropna(subset=['nongerman'])\n",
    "    return cp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "874d6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_cp_groups(universes, X_calib_f, X_calib_s, y_calib, \n",
    "                            X_test_f, X_test_s, y_test, alpha=0.1, models_dir=\"./models\"):\n",
    "    \"\"\"\n",
    "    Iterate over all universes and generate cp_groups DataFrame for each.\n",
    "    Returns a dictionary mapping universe ID to cp_groups DataFrame.\n",
    "    Also returns a combined DataFrame with an added 'universe_id' column for comparison (if needed).\n",
    "    \"\"\"\n",
    "    cp_groups_dict = {}\n",
    "    # Group universes by (model_type, feature_set) to reuse models/calibration\n",
    "    from collections import defaultdict\n",
    "    universe_groups = defaultdict(list)\n",
    "    for cfg in universes:\n",
    "        key = (cfg[\"model\"], cfg[\"feature_set\"])\n",
    "        universe_groups[key].append(cfg)\n",
    "    # Loop over each unique model configuration\n",
    "    for (model_type, feature_flag), cfg_list in universe_groups.items():\n",
    "        # Load the trained model for this config\n",
    "        model_path = f\"{models_dir}/{model_type}_{feature_flag}.joblib\"\n",
    "        model = load(model_path)\n",
    "        # Select the appropriate calibration and test sets based on feature_flag\n",
    "        X_calib = X_calib_f if feature_flag == \"with_protected\" else X_calib_s\n",
    "        X_test  = X_test_f  if feature_flag == \"with_protected\" else X_test_s\n",
    "        # Compute conformal prediction sets for this model\n",
    "        pred_sets = conformal_prediction_sets(model, X_calib, y_calib.to_numpy(), X_test, alpha)\n",
    "        # Build the base cp_groups DataFrame for this model (without universe ID yet)\n",
    "        cp_df_base = build_cp_groups(pred_sets, y_test, X_test.index, X_test_f)\n",
    "        # Assign this cp_groups DataFrame to each universe variant (threshold policy) in cfg_list\n",
    "        for cfg in cfg_list:\n",
    "            uid = cfg[\"id\"]\n",
    "            # We make a copy so each universe ID has an independent DataFrame (to avoid aliasing)\n",
    "            cp_groups_dict[uid] = cp_df_base.copy()\n",
    "            cp_groups_dict[uid][\"universe_id\"] = uid  # tag the universe ID (useful if combining)\n",
    "    # Combine all cp_groups into one DataFrame (optional)\n",
    "    combined_cp = pd.concat(cp_groups_dict.values(), ignore_index=True)\n",
    "    # Also add universe_id in combined (ensure present even if empty DataFrames were concatenated)\n",
    "    if \"universe_id\" not in combined_cp.columns:\n",
    "        combined_cp.insert(0, \"universe_id\", combined_cp.index.map(lambda i: list(cp_groups_dict.keys())[i] ))\n",
    "    return cp_groups_dict, combined_cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee88897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgroup_metrics(cp_df):\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, recall, and F1 for overall and each subgroup in cp_df.\n",
    "    Only single-label prediction sets are counted as definite predictions; \n",
    "    ambiguous sets (both classes) are ignored in metric calculations.\n",
    "    Returns a DataFrame with metrics for 'overall' and each subgroup.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Convert Series to list/array for processing\n",
    "    pred_sets_list = cp_df['pred_set'].tolist()\n",
    "    y_true = np.array(cp_df['true_label'])\n",
    "    # Boolean array for non-ambiguous predictions (prediction set of size 1):contentReference[oaicite:19]{index=19}\n",
    "    is_single = np.array([len(s) == 1 for s in pred_sets_list])\n",
    "    # Overall metrics (on non-ambiguous predictions only)\n",
    "    if np.any(is_single):\n",
    "        y_true_single = y_true[is_single]\n",
    "        # Extract the single predicted label from each singleton set\n",
    "        y_pred_single = [next(iter(s)) for s, flag in zip(pred_sets_list, is_single) if flag]\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_single, y_pred_single, labels=[0,1]).ravel()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true_single, y_pred_single, average=\"binary\", zero_division=0)\n",
    "        results[\"overall\"] = {\n",
    "            \"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn,\n",
    "            \"Accuracy\": (tp + tn) / (tp + tn + fp + fn) if (tp+tn+fp+fn) > 0 else 0.0,\n",
    "            \"Precision\": precision, \"Recall\": recall, \"F1\": f1,\n",
    "            \"Num_Samples\": len(y_true_single), \n",
    "            \"Frac_NonAmbiguous\": np.mean(is_single)  # fraction of test samples with a single-label prediction\n",
    "        }\n",
    "    else:\n",
    "        results[\"overall\"] = {\"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0,\"Accuracy\":0,\"Precision\":0,\"Recall\":0,\"F1\":0,\n",
    "                               \"Num_Samples\": 0, \"Frac_NonAmbiguous\": 0.0}\n",
    "    # Compute metrics for each subgroup (using only cases where subgroup flag == 1)\n",
    "    subgroup_cols = [\"frau1\", \"nongerman\", \"nongerman_male\", \"nongerman_female\"]\n",
    "    for col in subgroup_cols:\n",
    "        mask = (cp_df[col] == 1).to_numpy() & is_single  # only consider subgroup members with definite predictions\n",
    "        if np.any(mask):\n",
    "            y_true_sub = y_true[mask]\n",
    "            y_pred_sub = [next(iter(s)) for s, flag in zip(pred_sets_list, mask) if flag]\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true_sub, y_pred_sub, labels=[0,1]).ravel()\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_true_sub, y_pred_sub, average=\"binary\", zero_division=0)\n",
    "            results[col] = {\n",
    "                \"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn,\n",
    "                \"Accuracy\": (tp + tn) / (tp + tn + fp + fn) if (tp+tn+fp+fn) > 0 else 0.0,\n",
    "                \"Precision\": precision, \"Recall\": recall, \"F1\": f1,\n",
    "                \"Num_Samples\": len(y_true_sub),\n",
    "                \"Frac_NonAmbiguous\": np.mean(is_single[cp_df[col] == 1])  # fraction of subgroup's samples with singleton pred\n",
    "            }\n",
    "        else:\n",
    "            # No definite predictions for this subgroup (or subgroup empty after dropna)\n",
    "            results[col] = None\n",
    "    # Convert results to DataFrame for a tidy display\n",
    "    metrics_df = pd.DataFrame(results).T\n",
    "    metrics_df.index.name = 'Subgroup'\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a6665c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_groups_dict, cp_all_df = generate_all_cp_groups(universes, X_calib_f, X_calib_s, y_calib, \n",
    "                                                   X_test_f, X_test_s, y_test, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dd4855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    TP       TN     FP      FN  Accuracy  Precision    Recall  \\\n",
      "Subgroup                                                                        \n",
      "overall           32.0  67440.0  169.0  7433.0  0.898740   0.159204  0.004287   \n",
      "frau1             16.0  28079.0   77.0  3213.0  0.895173   0.172043  0.004955   \n",
      "nongerman         20.0  14412.0  156.0  1550.0  0.894287   0.113636  0.012739   \n",
      "nongerman_male    10.0   9417.0   84.0   798.0  0.914444   0.106383  0.012376   \n",
      "nongerman_female  10.0   4995.0   72.0   752.0  0.858638   0.121951  0.013123   \n",
      "\n",
      "                        F1  Num_Samples  Frac_NonAmbiguous  \n",
      "Subgroup                                                    \n",
      "overall           0.008349      75074.0           0.863088  \n",
      "frau1             0.009633      31385.0           0.846071  \n",
      "nongerman         0.022910      16138.0           0.905968  \n",
      "nongerman_male    0.022173      10309.0           0.928488  \n",
      "nongerman_female  0.023697       5829.0           0.868703  \n"
     ]
    }
   ],
   "source": [
    "# Get metrics for a specific universe (e.g., universe 3)\n",
    "metrics_u3 = compute_subgroup_metrics(cp_groups_dict[12])\n",
    "print(metrics_u3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "916c03e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h7/6qcvyjh51cg86vrxn3xs8c_40000gn/T/ipykernel_46865/1527154550.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  metrics_all = cp_all_df.groupby('universe_id').apply(lambda df: compute_subgroup_metrics(df))\n"
     ]
    }
   ],
   "source": [
    "# Or get metrics for all universes in one DataFrame by grouping cp_all_df\n",
    "metrics_all = cp_all_df.groupby('universe_id').apply(lambda df: compute_subgroup_metrics(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
