{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07550eba",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad59357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "import yaml\n",
    "\n",
    "from utils import (\n",
    "    compute_nc_scores,\n",
    "    find_threshold,\n",
    "    predict_conformal_sets,\n",
    "    evaluate_sets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394b1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load universe definitions from YAML\n",
    "with open(\"universes.yaml\") as f:\n",
    "    universes = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7871a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine unique model & feature_set combinations (each requires one model)\n",
    "unique_combos = {(cfg[\"model\"], cfg[\"feature_set\"]) for cfg in universes}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16090e24",
   "metadata": {},
   "source": [
    "## Data and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8eccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_f = pd.read_csv(\"./output/X_calib_f.csv\") # 2015, w. protected attributes\n",
    "X_calib_s = pd.read_csv(\"./output/X_calib_s.csv\") # 2015, w/o protected attributes\n",
    "y_calib = pd.read_csv(\"./output/y_calib.csv\").iloc[:,0]\n",
    "\n",
    "X_test_f = pd.read_csv(\"./output/X_test_f.csv\")\n",
    "X_test_s = pd.read_csv(\"./output/X_test_s.csv\")\n",
    "y_test = pd.read_csv(\"./output/y_test.csv\").iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ddb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from feature_set name to actual DataFrame, for convenience\n",
    "feature_sets_calib = {\n",
    "    \"with_protected\": X_calib_f,\n",
    "    \"without_protected\": X_calib_s\n",
    "}\n",
    "feature_sets_test = {\n",
    "    \"with_protected\": X_test_f,\n",
    "    \"without_protected\": X_test_s\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbfe25",
   "metadata": {},
   "source": [
    "## Conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34ec667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscoverage level for conformal prediction (10% allowed error rate => 90% target coverage)\n",
    "alpha = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac23896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: given a trained model file and data, produce conformal prediction sets and metrics\n",
    "def conformal_calibrate_and_evaluate(model_path, X_cal, y_cal, X_te, y_te, alpha):\n",
    "    \"\"\"Load model, compute conformal prediction sets on X_te using calibration set (X_cal, y_cal).\"\"\"\n",
    "    model = load(model_path)\n",
    "    # Compute nonconformity scores on calibration set (1 - probability of true class)\n",
    "    probs_cal = model.predict_proba(X_cal)\n",
    "    nc_scores = compute_nc_scores(probs_cal, y_cal)\n",
    "    # Find conformal threshold q_hat for the given alpha (split conformal method)\n",
    "    q_hat = find_threshold(nc_scores, alpha)\n",
    "    # Generate prediction sets for each test example\n",
    "    pred_sets = predict_conformal_sets(model, X_te, q_hat)\n",
    "    # Evaluate coverage and average set size on test data\n",
    "    metrics = evaluate_sets(pred_sets, y_te)\n",
    "    return q_hat, metrics, pred_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e0448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logreg (with_protected - Coverage: 0.911, Avg. Set Size: 1.13\n",
      "Model: logreg (without_protected - Coverage: 0.910, Avg. Set Size: 1.13\n",
      "Model: penalized_logreg (with_protected - Coverage: 0.911, Avg. Set Size: 1.13\n",
      "Model: penalized_logreg (without_protected - Coverage: 0.910, Avg. Set Size: 1.12\n",
      "Model: rf (with_protected - Coverage: 0.914, Avg. Set Size: 1.14\n",
      "Model: rf (without_protected - Coverage: 0.913, Avg. Set Size: 1.14\n"
     ]
    }
   ],
   "source": [
    "# Run conformal prediction for each model type & feature set combination\n",
    "conformal_results = {}  # to collect results for each universe combo\n",
    "for model_type, feature_flag in sorted(unique_combos):\n",
    "    # Identify the saved model file for this universe (matching training stage naming convention)\n",
    "    model_filename = f\"{model_type}_{feature_flag}.joblib\"\n",
    "    model_path = f\"./models/{model_filename}\"\n",
    "    # Select the corresponding calibration and test feature sets\n",
    "    X_cal = feature_sets_calib[feature_flag]\n",
    "    X_te  = feature_sets_test[feature_flag]\n",
    "    # Perform conformal calibration and evaluation\n",
    "    q_hat, metrics, pred_sets = conformal_calibrate_and_evaluate(model_path, X_cal, y_calib, X_te, y_test, alpha)\n",
    "    # Store results (coverage and average set size)\n",
    "    conformal_results[(model_type, feature_flag)] = {\n",
    "        \"q_hat\": q_hat,\n",
    "        \"coverage\": metrics[\"coverage\"],\n",
    "        \"avg_set_size\": metrics[\"avg_size\"]\n",
    "    }\n",
    "    # Print a summary of results for this model universe\n",
    "    cov = metrics[\"coverage\"]; avg = metrics[\"avg_size\"]\n",
    "    print(f\"Model: {model_type} ({feature_flag} - Coverage: {cov:.3f}, Avg. Set Size: {avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404cefef",
   "metadata": {},
   "source": [
    "## Analyzing CP per subgroup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "602ec344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "549cb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformal_prediction_sets(model, X_calib, y_calib, X_test, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Generate conformal prediction sets for X_test using calibration data (X_calib, y_calib).\n",
    "    Returns a list of prediction sets (as Python sets of class labels) for each test sample.\n",
    "    \"\"\"\n",
    "    # Compute nonconformity scores on calibration set (1 - probability of true class)\n",
    "    probs_calib = model.predict_proba(X_calib)         # shape: (n_calib, n_classes)\n",
    "    nc_scores = 1.0 - probs_calib[np.arange(len(y_calib)), y_calib]  # 1 - P(true_label)\n",
    "    # Determine conformal threshold q_hat at level alpha\n",
    "    q_hat = np.quantile(nc_scores, 1 - alpha, method=\"higher\")\n",
    "    # Compute nonconformity scores for each test sample and derive prediction sets\n",
    "    probs_test = model.predict_proba(X_test)           # shape: (n_test, n_classes)\n",
    "    nonconf_test = 1.0 - probs_test                    # nonconformity for each class\n",
    "    pred_sets = [ set(np.where(nc_row <= q_hat)[0]) for nc_row in nonconf_test ]\n",
    "    return pred_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "052db886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cp_groups(pred_sets, y_test, test_idx, X_test_full):\n",
    "    \"\"\"\n",
    "    Build a DataFrame of conformal prediction results with subgroup info.\n",
    "    - pred_sets: list of prediction sets for each test sample (in order of test_idx).\n",
    "    - y_test: Series or array of true labels for test samples.\n",
    "    - test_idx: index of the test samples corresponding to pred_sets.\n",
    "    - X_test_full: DataFrame of test features **including protected attributes**.\n",
    "    \"\"\"\n",
    "    cp_df = pd.DataFrame(index=test_idx.copy())\n",
    "    # Store prediction sets (convert each to a set of ints for consistency)\n",
    "    cp_df['pred_set']   = pd.Series(pred_sets, index=test_idx).apply(lambda s: {int(x) for x in s})\n",
    "    cp_df['true_label'] = y_test.reindex(test_idx)\n",
    "    # Bring in protected attributes from the full test set\n",
    "    cp_df['frau1']    = X_test_full.loc[test_idx, 'frau1']          # 1 = female, 0 = male\n",
    "    # Derive 'nongerman' flag ('maxdeutsch1' indicates German (1) vs non-German (0))\n",
    "    cp_df['nongerman'] = np.where(X_test_full.loc[test_idx, 'maxdeutsch1'] == 0, 1, 0)\n",
    "    # Handle missing nationality information if applicable\n",
    "    if 'maxdeutsch.Missing.' in X_test_full.columns:\n",
    "        missing_mask = (X_test_full.loc[test_idx, 'maxdeutsch.Missing.'] == 1)\n",
    "        cp_df.loc[missing_mask, 'nongerman'] = np.nan\n",
    "    # Derive intersectional subgroup flags\n",
    "    cp_df['nongerman_male']   = np.where((cp_df['nongerman'] == 1) & (cp_df['frau1'] == 0), 1, 0)\n",
    "    cp_df['nongerman_female'] = np.where((cp_df['nongerman'] == 1) & (cp_df['frau1'] == 1), 1, 0)\n",
    "    # (Optional) drop samples with missing subgroup info\n",
    "    cp_df = cp_df.dropna(subset=['nongerman'])\n",
    "    return cp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgroup_metrics(cp_df):\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, recall, and F1 for overall and each subgroup in cp_df.\n",
    "    Considers only single-label prediction sets as definite predictions.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Convert prediction sets to list for easier handling\n",
    "    pred_sets_list = cp_df['pred_set'].tolist()\n",
    "    y_true = np.array(cp_df['true_label'])\n",
    "    # Identify non-ambiguous (single-label) predictions\n",
    "    is_single = np.array([len(s) == 1 for s in pred_sets_list])\n",
    "    # Overall metrics for all single predictions\n",
    "    if np.any(is_single):\n",
    "        y_true_single = y_true[is_single]\n",
    "        # Extract the single predicted label from each singleton set\n",
    "        y_pred_single = [next(iter(s)) for s, flag in zip(pred_sets_list, is_single) if flag]\n",
    "        # Compute confusion matrix and metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_single, y_pred_single, labels=[0, 1]).ravel()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_single, y_pred_single, average=\"binary\", zero_division=0\n",
    "        )\n",
    "        results[\"overall\"] = {\n",
    "            \"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn,\n",
    "            \"Accuracy\": (tp + tn) / (tp + tn + fp + fn) if (tp+tn+fp+fn) > 0 else 0.0,\n",
    "            \"Precision\": precision, \"Recall\": recall, \"F1\": f1,\n",
    "            \"Num_Samples\": len(y_true_single),\n",
    "            \"Frac_NonAmbiguous\": np.mean(is_single)  # fraction of test samples with a single-label pred\n",
    "        }\n",
    "    else:\n",
    "        # If no single-label predictions, overall metrics are all zero/none\n",
    "        # ???? check if this is desired behavior\n",
    "        results[\"overall\"] = {\n",
    "            \"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0,\n",
    "            \"Accuracy\": 0.0, \"Precision\": 0.0, \"Recall\": 0.0, \"F1\": 0.0,\n",
    "            \"Num_Samples\": 0, \"Frac_NonAmbiguous\": 0.0\n",
    "        }\n",
    "    # List of subgroup columns to evaluate (1 indicates membership in subgroup)\n",
    "    subgroup_cols = [\"frau1\", \"nongerman\", \"nongerman_male\", \"nongerman_female\"]\n",
    "    for col in subgroup_cols:\n",
    "        mask = (cp_df[col] == 1).to_numpy() & is_single  # members of subgroup with definite predictions\n",
    "        if np.any(mask):\n",
    "            y_true_sub = y_true[mask]\n",
    "            y_pred_sub = [next(iter(s)) for s, flag in zip(pred_sets_list, mask) if flag]\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true_sub, y_pred_sub, labels=[0, 1]).ravel()\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                y_true_sub, y_pred_sub, average=\"binary\", zero_division=0\n",
    "            )\n",
    "            results[col] = {\n",
    "                \"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn,\n",
    "                \"Accuracy\": (tp + tn) / (tp + tn + fp + fn) if (tp+tn+fp+fn) > 0 else 0.0,\n",
    "                \"Precision\": precision, \"Recall\": recall, \"F1\": f1,\n",
    "                \"Num_Samples\": len(y_true_sub),\n",
    "                # Fraction of subgroup's samples with a single-label prediction\n",
    "                \"Frac_NonAmbiguous\": np.mean(is_single[cp_df[col] == 1])\n",
    "            }\n",
    "        else:\n",
    "            results[col] = None  # No definite predictions (or no samples) in this subgroup\n",
    "    metrics_df = pd.DataFrame(results).T\n",
    "    metrics_df.index.name = 'Subgroup'\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dd4855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TP       TN     FP      FN  Accuracy  Precision  \\\n",
      "Subgroup                                                               \n",
      "overall           345.0  68042.0  451.0  7305.0  0.898139   0.433417   \n",
      "frau1             118.0  28438.0  177.0  3187.0  0.894612   0.400000   \n",
      "nongerman          37.0  14876.0   36.0  1515.0  0.905794   0.506849   \n",
      "nongerman_male     22.0   9687.0   17.0   773.0  0.924755   0.564103   \n",
      "nongerman_female   15.0   5189.0   19.0   742.0  0.872422   0.441176   \n",
      "\n",
      "                    Recall        F1  Num_Samples  Frac_NonAmbiguous  \n",
      "Subgroup                                                              \n",
      "overall           0.045098  0.081695      76143.0           0.875378  \n",
      "frau1             0.035703  0.065556      31920.0           0.860493  \n",
      "nongerman         0.023840  0.045538      16464.0           0.924269  \n",
      "nongerman_male    0.027673  0.052758      10499.0           0.945600  \n",
      "nongerman_female  0.019815  0.037927       5965.0           0.888972  \n"
     ]
    }
   ],
   "source": [
    "# 2. Use the above functions in the analysis workflow\n",
    "# Example for a single universe or model configuration:\n",
    "model_path = \"./models/logreg_with_protected.joblib\"\n",
    "model = load(model_path)\n",
    "# Use appropriate feature sets depending on model (with or without protected)\n",
    "X_calib = X_calib_f   # (assuming this model uses protected attributes)\n",
    "X_test  = X_test_f\n",
    "# Generate conformal prediction sets for this model\n",
    "pred_sets = conformal_prediction_sets(model, X_calib, y_calib.to_numpy(), X_test, alpha=0.1)\n",
    "# Build the cp_groups DataFrame with true labels and group info\n",
    "cp_groups_df = build_cp_groups(pred_sets, y_test, X_test.index, X_test_f)\n",
    "# Compute subgroup metrics\n",
    "metrics = compute_subgroup_metrics(cp_groups_df)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute metrics for each universe in universes\n",
    "metrics_by_universe = {}\n",
    "records = []\n",
    "\n",
    "for universe in universes:\n",
    "    model_type = universe['model']\n",
    "    feature_flag = universe['feature_set']\n",
    "    universe_id = f\"{model_type}_{feature_flag}\"\n",
    "    model_path = f\"./models/{universe_id}.joblib\"\n",
    "    model = load(model_path)\n",
    "\n",
    "    if feature_flag == \"with_protected\":\n",
    "        X_calib, X_test = X_calib_f, X_test_f\n",
    "    else:\n",
    "        X_calib, X_test = X_calib_s, X_test_s\n",
    "\n",
    "    pred_sets = conformal_prediction_sets(model, X_calib, y_calib.to_numpy(), X_test, alpha=0.1)\n",
    "    cp_groups_df = build_cp_groups(pred_sets, y_test, X_test.index, X_test_f)\n",
    "    metrics = compute_subgroup_metrics(cp_groups_df)\n",
    "    metrics_by_universe[universe['id']] = metrics\n",
    "\n",
    "    for subgroup, row in metrics.iterrows():\n",
    "        if row is not None:\n",
    "            row_dict = row.to_dict()\n",
    "            row_dict.update({\n",
    "                \"UniverseID\": universe['id'],\n",
    "                \"Model\": model_type,\n",
    "                \"FeatureSet\": feature_flag,\n",
    "                \"Subgroup\": subgroup\n",
    "            })\n",
    "            records.append(row_dict)\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "all_metrics_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "916c03e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_df.to_csv(\"./output/conformal_subgroup_metrics.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
