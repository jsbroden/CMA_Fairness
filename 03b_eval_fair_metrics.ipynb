{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86f7a86",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632f326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from matplotlib.lines import Line2D\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fb48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, make_scorer, roc_curve, auc, precision_recall_curve, classification_report, confusion_matrix, accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748fb7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad62190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "from utils import aif_test, aif_plot, aif_plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35adfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f = pd.read_csv(\"./output/X_train_f.csv\")\n",
    "X_train_s = pd.read_csv(\"./output/X_train_s.csv\")\n",
    "\n",
    "X_test_f = pd.read_csv(\"./output/X_test_f.csv\")\n",
    "X_test_s = pd.read_csv(\"./output/X_test_s.csv\")\n",
    "y_test = pd.read_csv(\"./output/y_test.csv\")\n",
    "\n",
    "preds_test = pd.read_csv(\"./output/preds_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc40a6",
   "metadata": {},
   "source": [
    "## Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d4d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test = pd.concat([preds_test, X_test_f], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73ceec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test['nongerman'] = np.where(comb_test['maxdeutsch1'] == 0, 1, 0)\n",
    "comb_test.loc[comb_test['maxdeutsch.Missing.'] == 1, 'nongerman'] = np.nan\n",
    "comb_test['nongerman_male'] = np.where((comb_test['nongerman'] == 1) & (comb_test['frau1'] == 0), 1, 0)\n",
    "comb_test['nongerman_female'] = np.where((comb_test['nongerman'] == 1) & (comb_test['frau1'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2a1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test = comb_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "nongerman",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_test",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "47382855-db61-4bef-aa69-8fa58f83e5dd",
       "rows": [
        [
         "0.0",
         "0.13213821020673702"
        ],
        [
         "1.0",
         "0.1104249705271431"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nongerman</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.132138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.110425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test\n",
       "nongerman          \n",
       "0.0        0.132138\n",
       "1.0        0.110425"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computes the mean of y_test for each value of nongerman\n",
    "# Interpreted as the base rate (i.e., unemployment rate) among germans and nongermans\n",
    "comb_test[['y_test', 'nongerman']].groupby(['nongerman']).mean() # Baseline\n",
    "\n",
    "#comb_test[['rf2_c1', 'nongerman']].groupby(['nongerman']).mean() # High risk (w/o protected attributes)\n",
    "#comb_test[['rf2_c2', 'nongerman']].groupby(['nongerman']).mean() # High risk (w/o protected attributes)\n",
    "#comb_test[['rf2_c3', 'nongerman']].groupby(['nongerman']).mean() # Middle risk (w/o protected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29f02ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "y_test",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "frau1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nongerman",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nongerman_male",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nongerman_female",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "811e0b95-62e7-414d-a767-291f3047ddfe",
       "rows": [
        [
         "0",
         "0.42367810638410036",
         "0.2088407401549897",
         "0.13363909536612367",
         "0.07520164478886604"
        ],
        [
         "1",
         "0.4454848293868731",
         "0.17709552534437742",
         "0.08670207976951472",
         "0.0903934455748627"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frau1</th>\n",
       "      <th>nongerman</th>\n",
       "      <th>nongerman_male</th>\n",
       "      <th>nongerman_female</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.423678</td>\n",
       "      <td>0.208841</td>\n",
       "      <td>0.133639</td>\n",
       "      <td>0.075202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.445485</td>\n",
       "      <td>0.177096</td>\n",
       "      <td>0.086702</td>\n",
       "      <td>0.090393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           frau1  nongerman  nongerman_male  nongerman_female\n",
       "y_test                                                       \n",
       "0       0.423678   0.208841        0.133639          0.075202\n",
       "1       0.445485   0.177096        0.086702          0.090393"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_test[['y_test', 'frau1', 'nongerman', 'nongerman_male', 'nongerman_female']].groupby(['y_test']).mean() # Baseline\n",
    "#comb_test[['rf2_c1', 'frau1', 'nongerman', 'nongerman_male', 'nongerman_female']].groupby(['rf2_c1']).mean() # High risk (w/o protected attributes)\n",
    "#comb_test[['rf2_c2', 'frau1', 'nongerman', 'nongerman_male', 'nongerman_female']].groupby(['rf2_c2']).mean() # High risk (w/o protected attributes)\n",
    "#comb_test[['rf2_c3', 'frau1', 'nongerman', 'nongerman_male', 'nongerman_female']].groupby(['rf2_c3']).mean() # Middle risk (w/o protected attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e9c8a",
   "metadata": {},
   "source": [
    "# 01 Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0323af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test_s = pd.concat([y_test, X_test_s], axis = 1) # w/o protected attributes\n",
    "preds_test_s = preds_test\n",
    "\n",
    "label_test = pd.concat([y_test, X_test_f], axis = 1) # with protected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88bac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test.loc[label_test['maxdeutsch.Missing.'] == 1, 'maxdeutsch1'] = np.nan\n",
    "preds_test.loc[label_test['maxdeutsch.Missing.'] == 1, 'y_test'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff56f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test['nongerman'] = np.where(label_test['maxdeutsch1'] == 0, 1, 0)\n",
    "label_test['nongerman_male'] = np.where((label_test['nongerman'] == 1) & (label_test['frau1'] == 0), 1, 0)\n",
    "label_test['nongerman_female'] = np.where((label_test['nongerman'] == 1) & (label_test['frau1'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651c5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = label_test.dropna().reset_index(drop = True)\n",
    "preds_test = preds_test.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd9a92",
   "metadata": {},
   "source": [
    "# 01 Stat. Parity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0498d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Fairness for observed label\n",
    "\n",
    "protected_attribute = ['frau1']\n",
    "unprivileged_group = [{'frau1': 1}]\n",
    "privileged_group = [{'frau1': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wraps pandas df into aif360 BinaryLabelDataset (required for computing f metrics with aif360 lib)\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes baseline f metrics for true labels (how fair/unfair is world already before applying any model)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab441b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPD for sex attribute\n",
    "# interpretation: \n",
    "# - 0 equal positive outcome reates for both groups\n",
    "# - < 0 (negative) unpriviliged group get fewer positive outcomes\n",
    "# - > 0 (positive) unpriviliged group get more positive outcomes\n",
    "\n",
    "base_par_sex = metric_test_label.statistical_parity_difference() # Label diff female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b13f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['maxdeutsch1']\n",
    "unprivileged_group = [{'maxdeutsch1': 0}]\n",
    "privileged_group = [{'maxdeutsch1': 1}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_par_ger = metric_test_label.statistical_parity_difference() # Label diff nongerman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71bed2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['nongerman_male']\n",
    "unprivileged_group = [{'nongerman_male': 1}]\n",
    "privileged_group = [{'nongerman_male': 0}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_par_ger_male = metric_test_label.statistical_parity_difference() # Label diff nongerman male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41e7584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['nongerman_female']\n",
    "unprivileged_group = [{'nongerman_female': 1}]\n",
    "privileged_group = [{'nongerman_female': 0}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_par_ger_female = metric_test_label.statistical_parity_difference() # Label diff nongerman female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8d076",
   "metadata": {},
   "source": [
    "# Loop over models (w protected attributes) and cutoffs to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617f5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n"
     ]
    }
   ],
   "source": [
    "# calculates SPD across several protected groups for a list of models and stores results in fairness1\n",
    "\n",
    "fairness1 = []\n",
    "\n",
    "for column in preds_test[['glm1_c1', 'glm1_c2', 'glm1_c3']]: # ,'net1_c1', 'net1_c2', 'net1_c3','rf1_c1', 'rf1_c2', 'rf1_c3','gbm1_c1', 'gbm1_c2', 'gbm1_c3'\n",
    "\n",
    "    protected_attribute = ['frau1']\n",
    "    unprivileged_group = [{'frau1': 1}]\n",
    "    privileged_group = [{'frau1': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    pred = preds_test[column]\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_sex = metric_test_pred.statistical_parity_difference() # Parity difference for female\n",
    "    \n",
    "    protected_attribute = ['maxdeutsch1']\n",
    "    unprivileged_group = [{'maxdeutsch1': 0}]\n",
    "    privileged_group = [{'maxdeutsch1': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman\n",
    "    \n",
    "    protected_attribute = ['nongerman_male']\n",
    "    unprivileged_group = [{'nongerman_male': 1}]\n",
    "    privileged_group = [{'nongerman_male': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_male = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman male\n",
    "    \n",
    "    protected_attribute = ['nongerman_female']\n",
    "    unprivileged_group = [{'nongerman_female': 1}]\n",
    "    privileged_group = [{'nongerman_female': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_female = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman female\n",
    "    \n",
    "    fairness1.append([column,\n",
    "                     par_sex,\n",
    "                     par_ger,\n",
    "                     par_ger_male,\n",
    "                     par_ger_female])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98693fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness1 = pd.DataFrame(fairness1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new row at the top with SPD calculated from the true labels\n",
    "fairness1.loc[-1] = ['label', base_par_sex, base_par_ger, base_par_ger_male, base_par_ger_female]\n",
    "fairness1 = fairness1.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba36cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness1 = fairness1.rename(columns={0: \"Model\", 1: \"Parity Diff. (Female)\", 2: \"Parity Diff. (Non-German)\", 3: \"Parity Diff. (Non-German-Male)\", 4: \"Parity Diff. (Non-German-Female)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a732773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness1.to_latex('./output/test_fairness1.tex', index = False, float_format = \"%.3f\")\n",
    "fairness1.to_csv('./output/test_fairness1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214bab1",
   "metadata": {},
   "source": [
    "# Loop over models (w/o protected attributes) and cutoffs to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip for now, not calculated yet w/o protected attributes\n",
    "\n",
    "fairness2 = []\n",
    "\n",
    "for column in preds_test[['glm2_c1', 'glm2_c2', 'glm2_c3',\n",
    "                          'net2_c1', 'net2_c2', 'net2_c3',\n",
    "                          'rf2_c1', 'rf2_c2', 'rf2_c3',\n",
    "                          'gbm2_c1', 'gbm2_c2', 'gbm2_c3']]:\n",
    "\n",
    "    protected_attribute = ['frau1']\n",
    "    unprivileged_group = [{'frau1': 1}]\n",
    "    privileged_group = [{'frau1': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    pred = preds_test[column]\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_sex = metric_test_pred.statistical_parity_difference() # Parity difference for female\n",
    "    \n",
    "    protected_attribute = ['maxdeutsch1']\n",
    "    unprivileged_group = [{'maxdeutsch1': 0}]\n",
    "    privileged_group = [{'maxdeutsch1': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman\n",
    "    \n",
    "    protected_attribute = ['nongerman_male']\n",
    "    unprivileged_group = [{'nongerman_male': 1}]\n",
    "    privileged_group = [{'nongerman_male': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_male = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman male\n",
    "    \n",
    "    protected_attribute = ['nongerman_female']\n",
    "    unprivileged_group = [{'nongerman_female': 1}]\n",
    "    privileged_group = [{'nongerman_female': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_female = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman female\n",
    "    \n",
    "    fairness2.append([column,\n",
    "                     par_sex,\n",
    "                     par_ger,\n",
    "                     par_ger_male,\n",
    "                     par_ger_female])\n",
    "\n",
    "fairness2 = pd.DataFrame(fairness2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da73a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip for now, not calculated yet w/o protected attributes\n",
    "\n",
    "fairness2.loc[-1] = ['label', base_par_sex, base_par_ger, base_par_ger_male, base_par_ger_female]\n",
    "fairness2 = fairness2.sort_index()\n",
    "\n",
    "fairness2 = fairness2.rename(columns={0: \"Model\", 1: \"Parity Diff. (Female)\", 2: \"Parity Diff. (Non-German)\", 3: \"Parity Diff. (Non-German-Male)\", 4: \"Parity Diff. (Non-German-Female)\"})\n",
    "\n",
    "fairness2.to_latex('./output/test_fairness2.tex', index = False, float_format = \"%.3f\")\n",
    "fairness2.to_csv('./output/test_fairness2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3598099",
   "metadata": {},
   "source": [
    "# 02: Cond. Stat. Parity Difference (Edu = Abitur) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9946eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Fairness for observed label\n",
    "\n",
    "protected_attribute = ['frau1', 'maxschule9']\n",
    "unprivileged_group = [{'frau1': 1, 'maxschule9': 1}]\n",
    "privileged_group = [{'frau1': 0, 'maxschule9': 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4ee29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5063d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0aefb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cpar_sex = metric_test_label.statistical_parity_difference() # Label diff female (edu = abi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66ab3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['maxdeutsch1', 'maxschule9']\n",
    "unprivileged_group = [{'maxdeutsch1': 0, 'maxschule9': 1}]\n",
    "privileged_group = [{'maxdeutsch1': 1, 'maxschule9': 1}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_cpar_ger = metric_test_label.statistical_parity_difference() # Label diff nongerman (edu = abi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e45e6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['nongerman_male', 'maxschule9']\n",
    "unprivileged_group = [{'nongerman_male': 1, 'maxschule9': 1}]\n",
    "privileged_group = [{'nongerman_male': 0, 'maxschule9': 1}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_cpar_ger_male = metric_test_label.statistical_parity_difference() # Label diff nongerman male (edu = abi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5d71506",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['nongerman_female', 'maxschule9']\n",
    "unprivileged_group = [{'nongerman_female': 1, 'maxschule9': 1}]\n",
    "privileged_group = [{'nongerman_female': 0, 'maxschule9': 1}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_cpar_ger_female = metric_test_label.statistical_parity_difference() # Label diff nongerman female (edu = abi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee68435",
   "metadata": {},
   "source": [
    "# Loop over models (w protected attributes) and cutoffs to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d701fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n",
      "/Users/julia/Desktop/CMA_Fairness/cma_f/lib/python3.11/site-packages/aif360/metrics/utils.py:78: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y = y.ravel()\n"
     ]
    }
   ],
   "source": [
    "cond_fair1 = []\n",
    "\n",
    "for column in preds_test[['glm1_c1', 'glm1_c2', 'glm1_c3']]: # ,'net1_c1', 'net1_c2', 'net1_c3','rf1_c1', 'rf1_c2', 'rf1_c3','gbm1_c1', 'gbm1_c2', 'gbm1_c3'\n",
    "\n",
    "    protected_attribute = ['frau1', 'maxschule9']\n",
    "    unprivileged_group = [{'frau1': 1, 'maxschule9': 1}]\n",
    "    privileged_group = [{'frau1': 0, 'maxschule9': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    pred = preds_test[column]\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "\n",
    "    par_sex = metric_test_pred.statistical_parity_difference() # Parity difference for female (edu = abi)\n",
    "    \n",
    "    protected_attribute = ['maxdeutsch1', 'maxschule9']\n",
    "    unprivileged_group = [{'maxdeutsch1': 0, 'maxschule9': 1}]\n",
    "    privileged_group = [{'maxdeutsch1': 1, 'maxschule9': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman (edu = abi)\n",
    "    \n",
    "    protected_attribute = ['nongerman_male', 'maxschule9']\n",
    "    unprivileged_group = [{'nongerman_male': 1, 'maxschule9': 1}]\n",
    "    privileged_group = [{'nongerman_male': 0, 'maxschule9': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_male = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman male (edu = abi)\n",
    "    \n",
    "    protected_attribute = ['nongerman_female', 'maxschule9']\n",
    "    unprivileged_group = [{'nongerman_female': 1, 'maxschule9': 1}]\n",
    "    privileged_group = [{'nongerman_female': 0, 'maxschule9': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_female = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman female (edu = abi)\n",
    "    \n",
    "    cond_fair1.append([column,\n",
    "                      par_sex,\n",
    "                      par_ger,\n",
    "                      par_ger_male,\n",
    "                      par_ger_female])\n",
    "\n",
    "cond_fair1 = pd.DataFrame(cond_fair1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dac672c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_fair1.loc[-1] = ['label', base_cpar_sex, base_cpar_ger, base_cpar_ger_male, base_cpar_ger_female]\n",
    "cond_fair1 = cond_fair1.sort_index()\n",
    "\n",
    "cond_fair1 = cond_fair1.rename(columns={0: \"Model\", 1: \"Cond. Parity Diff. (Female)\", 2: \"Cond. Parity Diff. (Non-German)\", 3: \"Cond. Parity Diff. (Non-German-Male)\", 4: \"Cond. Parity Diff. (Non-German-Female)\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73555bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_fair1.to_latex('./output/test_cond_fairness1.tex', index = False, float_format = \"%.3f\")\n",
    "cond_fair1.to_csv('./output/test_cond_fairness1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036dd1d",
   "metadata": {},
   "source": [
    "# Loop over models (w/o protected attributes) and cutoffs to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... skip for now, not calculated yet w/o protected attributes yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c054438",
   "metadata": {},
   "source": [
    "# Combine all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bbe13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness1 = pd.read_csv(\"./output/test_fairness1.csv\")\n",
    "cond_fair1 = pd.read_csv(\"./output/test_cond_fairness1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38f43f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_fair1 = cond_fair1.drop(columns={'Model'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf16e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_fair1 = pd.concat([fairness1,\n",
    "                             cond_fair1],\n",
    "                            axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9520481",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_fair1.to_latex('./output/test_full_fairness1.tex', index = False, float_format = \"%.2f\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
