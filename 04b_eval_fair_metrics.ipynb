{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86f7a86",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from matplotlib.lines import Line2D\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, make_scorer, roc_curve, auc, precision_recall_curve, classification_report, confusion_matrix, accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad62190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import aif_test, aif_plot, aif_plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35adfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f = pd.read_csv(\"./output/X_train_f.csv\")\n",
    "X_train_s = pd.read_csv(\"./output/X_train_s.csv\")\n",
    "\n",
    "X_test_f = pd.read_csv(\"./output/X_test_f.csv\")\n",
    "X_test_s = pd.read_csv(\"./output/X_test_s.csv\")\n",
    "y_test = pd.read_csv(\"./output/y_test.csv\")\n",
    "\n",
    "preds_test = pd.read_csv(\"./output/preds_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc40a6",
   "metadata": {},
   "source": [
    "## Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test = pd.concat([preds_test, X_test_f], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ceec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test['nongerman'] = np.where(comb_test['maxdeutsch1'] == 0, 1, 0)\n",
    "comb_test.loc[comb_test['maxdeutsch.Missing.'] == 1, 'nongerman'] = np.nan\n",
    "comb_test['nongerman_male'] = np.where((comb_test['nongerman'] == 1) & (comb_test['frau1'] == 0), 1, 0)\n",
    "comb_test['nongerman_female'] = np.where((comb_test['nongerman'] == 1) & (comb_test['frau1'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test = comb_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the mean of y_test for each value of nongerman\n",
    "# Interpreted as the base rate (i.e., unemployment rate) among germans and nongermans\n",
    "comb_test[['y_test', 'nongerman']].groupby(['nongerman']).mean() # Baseline\n",
    "\n",
    "#comb_test[['rf2_c1', 'nongerman']].groupby(['nongerman']).mean() # High risk (w/o protected attributes)\n",
    "#comb_test[['rf2_c2', 'nongerman']].groupby(['nongerman']).mean() # High risk (w/o protected attributes)\n",
    "#comb_test[['rf2_c3', 'nongerman']].groupby(['nongerman']).mean() # Middle risk (w/o protected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f02ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test[['y_test', 'frau1', 'nongerman', 'nongerman_male', 'nongerman_female']].groupby(['y_test']).mean() # Baseline\n",
    "#comb_test[['rf2_c1', 'frau1', 'nongerman', 'nongerman_male', 'nongerman_female']].groupby(['rf2_c1']).mean() # High risk (w/o protected attributes)\n",
    "#comb_test[['rf2_c2', 'frau1', 'nongerman', 'nongerman_male', 'nongerman_female']].groupby(['rf2_c2']).mean() # High risk (w/o protected attributes)\n",
    "#comb_test[['rf2_c3', 'frau1', 'nongerman', 'nongerman_male', 'nongerman_female']].groupby(['rf2_c3']).mean() # Middle risk (w/o protected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test.to_csv('./output/comb_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e9c8a",
   "metadata": {},
   "source": [
    "# 01 Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test_s = pd.concat([y_test, X_test_s], axis = 1) # w/o protected attributes\n",
    "preds_test_s = preds_test\n",
    "\n",
    "label_test = pd.concat([y_test, X_test_f], axis = 1) # with protected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test.loc[label_test['maxdeutsch.Missing.'] == 1, 'maxdeutsch1'] = np.nan\n",
    "preds_test.loc[label_test['maxdeutsch.Missing.'] == 1, 'y_test'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test['nongerman'] = np.where(label_test['maxdeutsch1'] == 0, 1, 0)\n",
    "label_test['nongerman_male'] = np.where((label_test['nongerman'] == 1) & (label_test['frau1'] == 0), 1, 0)\n",
    "label_test['nongerman_female'] = np.where((label_test['nongerman'] == 1) & (label_test['frau1'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = label_test.dropna().reset_index(drop = True)\n",
    "preds_test = preds_test.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd9a92",
   "metadata": {},
   "source": [
    "# 01 Stat. Parity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0498d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Fairness for observed label\n",
    "\n",
    "protected_attribute = ['frau1']\n",
    "unprivileged_group = [{'frau1': 1}]\n",
    "privileged_group = [{'frau1': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wraps pandas df into aif360 BinaryLabelDataset (required for computing f metrics with aif360 lib)\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes baseline f metrics for true labels (how fair/unfair is world already before applying any model)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab441b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPD for sex attribute\n",
    "# interpretation: \n",
    "# - 0 equal positive outcome reates for both groups\n",
    "# - < 0 (negative) unpriviliged group get fewer positive outcomes\n",
    "# - > 0 (positive) unpriviliged group get more positive outcomes\n",
    "\n",
    "base_par_sex = metric_test_label.statistical_parity_difference() # Label diff female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['maxdeutsch1']\n",
    "unprivileged_group = [{'maxdeutsch1': 0}]\n",
    "privileged_group = [{'maxdeutsch1': 1}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_par_ger = metric_test_label.statistical_parity_difference() # Label diff nongerman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bed2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['nongerman_male']\n",
    "unprivileged_group = [{'nongerman_male': 1}]\n",
    "privileged_group = [{'nongerman_male': 0}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_par_ger_male = metric_test_label.statistical_parity_difference() # Label diff nongerman male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['nongerman_female']\n",
    "unprivileged_group = [{'nongerman_female': 1}]\n",
    "privileged_group = [{'nongerman_female': 0}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_par_ger_female = metric_test_label.statistical_parity_difference() # Label diff nongerman female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8d076",
   "metadata": {},
   "source": [
    "# Loop over models (w protected attributes) and cutoffs to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates SPD across several protected groups for a list of models and stores results in fairness1\n",
    "\n",
    "fairness1 = []\n",
    "\n",
    "for column in preds_test[['glm1_c1', 'glm1_c2', 'glm1_c3']]: # ,'net1_c1', 'net1_c2', 'net1_c3','rf1_c1', 'rf1_c2', 'rf1_c3','gbm1_c1', 'gbm1_c2', 'gbm1_c3'\n",
    "\n",
    "    protected_attribute = ['frau1']\n",
    "    unprivileged_group = [{'frau1': 1}]\n",
    "    privileged_group = [{'frau1': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    pred = preds_test[column]\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_sex = metric_test_pred.statistical_parity_difference() # Parity difference for female\n",
    "    \n",
    "    protected_attribute = ['maxdeutsch1']\n",
    "    unprivileged_group = [{'maxdeutsch1': 0}]\n",
    "    privileged_group = [{'maxdeutsch1': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman\n",
    "    \n",
    "    protected_attribute = ['nongerman_male']\n",
    "    unprivileged_group = [{'nongerman_male': 1}]\n",
    "    privileged_group = [{'nongerman_male': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_male = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman male\n",
    "    \n",
    "    protected_attribute = ['nongerman_female']\n",
    "    unprivileged_group = [{'nongerman_female': 1}]\n",
    "    privileged_group = [{'nongerman_female': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_female = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman female\n",
    "    \n",
    "    fairness1.append([column,\n",
    "                     par_sex,\n",
    "                     par_ger,\n",
    "                     par_ger_male,\n",
    "                     par_ger_female])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98693fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness1 = pd.DataFrame(fairness1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new row at the top with SPD calculated from the true labels\n",
    "fairness1.loc[-1] = ['label', base_par_sex, base_par_ger, base_par_ger_male, base_par_ger_female]\n",
    "fairness1 = fairness1.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba36cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness1 = fairness1.rename(columns={0: \"Model\", 1: \"Parity Diff. (Female)\", 2: \"Parity Diff. (Non-German)\", 3: \"Parity Diff. (Non-German-Male)\", 4: \"Parity Diff. (Non-German-Female)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness1.to_latex('./output/test_fairness1.tex', index = False, float_format = \"%.3f\")\n",
    "fairness1.to_csv('./output/test_fairness1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214bab1",
   "metadata": {},
   "source": [
    "# Loop over models (w/o protected attributes) and cutoffs to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip for now, not calculated yet w/o protected attributes\n",
    "\n",
    "fairness2 = []\n",
    "\n",
    "for column in preds_test[['glm2_c1', 'glm2_c2', 'glm2_c3',\n",
    "                          'net2_c1', 'net2_c2', 'net2_c3',\n",
    "                          'rf2_c1', 'rf2_c2', 'rf2_c3',\n",
    "                          'gbm2_c1', 'gbm2_c2', 'gbm2_c3']]:\n",
    "\n",
    "    protected_attribute = ['frau1']\n",
    "    unprivileged_group = [{'frau1': 1}]\n",
    "    privileged_group = [{'frau1': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    pred = preds_test[column]\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_sex = metric_test_pred.statistical_parity_difference() # Parity difference for female\n",
    "    \n",
    "    protected_attribute = ['maxdeutsch1']\n",
    "    unprivileged_group = [{'maxdeutsch1': 0}]\n",
    "    privileged_group = [{'maxdeutsch1': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman\n",
    "    \n",
    "    protected_attribute = ['nongerman_male']\n",
    "    unprivileged_group = [{'nongerman_male': 1}]\n",
    "    privileged_group = [{'nongerman_male': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_male = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman male\n",
    "    \n",
    "    protected_attribute = ['nongerman_female']\n",
    "    unprivileged_group = [{'nongerman_female': 1}]\n",
    "    privileged_group = [{'nongerman_female': 0}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_female = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman female\n",
    "    \n",
    "    fairness2.append([column,\n",
    "                     par_sex,\n",
    "                     par_ger,\n",
    "                     par_ger_male,\n",
    "                     par_ger_female])\n",
    "\n",
    "fairness2 = pd.DataFrame(fairness2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da73a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip for now, not calculated yet w/o protected attributes\n",
    "\n",
    "fairness2.loc[-1] = ['label', base_par_sex, base_par_ger, base_par_ger_male, base_par_ger_female]\n",
    "fairness2 = fairness2.sort_index()\n",
    "\n",
    "fairness2 = fairness2.rename(columns={0: \"Model\", 1: \"Parity Diff. (Female)\", 2: \"Parity Diff. (Non-German)\", 3: \"Parity Diff. (Non-German-Male)\", 4: \"Parity Diff. (Non-German-Female)\"})\n",
    "\n",
    "fairness2.to_latex('./output/test_fairness2.tex', index = False, float_format = \"%.3f\")\n",
    "fairness2.to_csv('./output/test_fairness2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3598099",
   "metadata": {},
   "source": [
    "# 02: Cond. Stat. Parity Difference (Edu = Abitur) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9946eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Fairness for observed label\n",
    "\n",
    "protected_attribute = ['frau1', 'maxschule9']\n",
    "unprivileged_group = [{'frau1': 1, 'maxschule9': 1}]\n",
    "privileged_group = [{'frau1': 0, 'maxschule9': 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aefb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cpar_sex = metric_test_label.statistical_parity_difference() # Label diff female (edu = abi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['maxdeutsch1', 'maxschule9']\n",
    "unprivileged_group = [{'maxdeutsch1': 0, 'maxschule9': 1}]\n",
    "privileged_group = [{'maxdeutsch1': 1, 'maxschule9': 1}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_cpar_ger = metric_test_label.statistical_parity_difference() # Label diff nongerman (edu = abi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['nongerman_male', 'maxschule9']\n",
    "unprivileged_group = [{'nongerman_male': 1, 'maxschule9': 1}]\n",
    "privileged_group = [{'nongerman_male': 0, 'maxschule9': 1}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_cpar_ger_male = metric_test_label.statistical_parity_difference() # Label diff nongerman male (edu = abi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d71506",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = ['nongerman_female', 'maxschule9']\n",
    "unprivileged_group = [{'nongerman_female': 1, 'maxschule9': 1}]\n",
    "privileged_group = [{'nongerman_female': 0, 'maxschule9': 1}]\n",
    "\n",
    "test_label = BinaryLabelDataset(df = label_test,\n",
    "                                label_names = ['ltue'], \n",
    "                                protected_attribute_names = protected_attribute)\n",
    "\n",
    "metric_test_label = BinaryLabelDatasetMetric(test_label, \n",
    "                                             unprivileged_groups = unprivileged_group,\n",
    "                                             privileged_groups = privileged_group)\n",
    "\n",
    "base_cpar_ger_female = metric_test_label.statistical_parity_difference() # Label diff nongerman female (edu = abi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee68435",
   "metadata": {},
   "source": [
    "# Loop over models (w protected attributes) and cutoffs to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d701fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_fair1 = []\n",
    "\n",
    "for column in preds_test[['glm1_c1', 'glm1_c2', 'glm1_c3']]: # ,'net1_c1', 'net1_c2', 'net1_c3','rf1_c1', 'rf1_c2', 'rf1_c3','gbm1_c1', 'gbm1_c2', 'gbm1_c3'\n",
    "\n",
    "    protected_attribute = ['frau1', 'maxschule9']\n",
    "    unprivileged_group = [{'frau1': 1, 'maxschule9': 1}]\n",
    "    privileged_group = [{'frau1': 0, 'maxschule9': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    pred = preds_test[column]\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "\n",
    "    par_sex = metric_test_pred.statistical_parity_difference() # Parity difference for female (edu = abi)\n",
    "    \n",
    "    protected_attribute = ['maxdeutsch1', 'maxschule9']\n",
    "    unprivileged_group = [{'maxdeutsch1': 0, 'maxschule9': 1}]\n",
    "    privileged_group = [{'maxdeutsch1': 1, 'maxschule9': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman (edu = abi)\n",
    "    \n",
    "    protected_attribute = ['nongerman_male', 'maxschule9']\n",
    "    unprivileged_group = [{'nongerman_male': 1, 'maxschule9': 1}]\n",
    "    privileged_group = [{'nongerman_male': 0, 'maxschule9': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_male = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman male (edu = abi)\n",
    "    \n",
    "    protected_attribute = ['nongerman_female', 'maxschule9']\n",
    "    unprivileged_group = [{'nongerman_female': 1, 'maxschule9': 1}]\n",
    "    privileged_group = [{'nongerman_female': 0, 'maxschule9': 1}]\n",
    "    \n",
    "    test_label = BinaryLabelDataset(df = label_test,\n",
    "                                    label_names = ['ltue'], \n",
    "                                    protected_attribute_names = protected_attribute)\n",
    "\n",
    "    test_pred = test_label.copy()\n",
    "    test_pred.labels = pred\n",
    "    \n",
    "    metric_test_pred = BinaryLabelDatasetMetric(test_pred, \n",
    "                                                unprivileged_groups = unprivileged_group,\n",
    "                                                privileged_groups = privileged_group)\n",
    "    \n",
    "    par_ger_female = metric_test_pred.statistical_parity_difference() # Parity difference for nongerman female (edu = abi)\n",
    "    \n",
    "    cond_fair1.append([column,\n",
    "                      par_sex,\n",
    "                      par_ger,\n",
    "                      par_ger_male,\n",
    "                      par_ger_female])\n",
    "\n",
    "cond_fair1 = pd.DataFrame(cond_fair1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac672c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_fair1.loc[-1] = ['label', base_cpar_sex, base_cpar_ger, base_cpar_ger_male, base_cpar_ger_female]\n",
    "cond_fair1 = cond_fair1.sort_index()\n",
    "\n",
    "cond_fair1 = cond_fair1.rename(columns={0: \"Model\", 1: \"Cond. Parity Diff. (Female)\", 2: \"Cond. Parity Diff. (Non-German)\", 3: \"Cond. Parity Diff. (Non-German-Male)\", 4: \"Cond. Parity Diff. (Non-German-Female)\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73555bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_fair1.to_latex('./output/test_cond_fairness1.tex', index = False, float_format = \"%.3f\")\n",
    "cond_fair1.to_csv('./output/test_cond_fairness1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036dd1d",
   "metadata": {},
   "source": [
    "# Loop over models (w/o protected attributes) and cutoffs to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... skip for now, not calculated yet w/o protected attributes yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c054438",
   "metadata": {},
   "source": [
    "# Combine all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness1 = pd.read_csv(\"./output/test_fairness1.csv\")\n",
    "cond_fair1 = pd.read_csv(\"./output/test_cond_fairness1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f43f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_fair1 = cond_fair1.drop(columns={'Model'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_fair1 = pd.concat([fairness1,\n",
    "                             cond_fair1],\n",
    "                            axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9520481",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_fair1.to_latex('./output/test_full_fairness1.tex', index = False, float_format = \"%.2f\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
