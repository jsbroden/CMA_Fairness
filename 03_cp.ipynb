{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07550eba",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad59357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "\n",
    "from utils import (\n",
    "    compute_nc_scores,\n",
    "    find_threshold,\n",
    "    predict_conformal_sets,\n",
    "    evaluate_sets,\n",
    "    summarize_by_indicator,\n",
    "    summarize_for_predicate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16090e24",
   "metadata": {},
   "source": [
    "## Data and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8eccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib_f = pd.read_csv(\"./output/X_calib_f.csv\") # 2015, w. protected attributes\n",
    "X_calib_s = pd.read_csv(\"./output/X_calib_s.csv\") # 2015, w/o protected attributes\n",
    "y_calib = pd.read_csv(\"./output/y_calib.csv\").iloc[:,0]\n",
    "\n",
    "X_test_f = pd.read_csv(\"./output/X_test_f.csv\")\n",
    "X_test_s = pd.read_csv(\"./output/X_test_s.csv\")\n",
    "y_test = pd.read_csv(\"./output/y_test.csv\").iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_test = pd.read_csv(\"./output/preds_test.csv\")\n",
    "\n",
    "glm1 = load(\"./models/glm1.joblib\")\n",
    "glm2 = load(\"./models/glm2.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbfe25",
   "metadata": {},
   "source": [
    "## Conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscoverage level\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907a640",
   "metadata": {},
   "source": [
    "### Conformal - Logit Regression (w. protected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_calib1 = glm1.predict_proba(X_calib_f)\n",
    "\n",
    "nc_scores1 = compute_nc_scores(probs_calib1, y_calib)\n",
    "\n",
    "q_hat1 = find_threshold(nc_scores1, alpha) # q_hat is data-driven threshold for classification\n",
    "print(f\"q_hat1: {q_hat1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be08fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With test data\n",
    "pred_sets1 = predict_conformal_sets(glm1, X_test_f, q_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With test data\n",
    "evaluation1 = evaluate_sets(pred_sets1, y_test)\n",
    "print(f\"Coverage1: {evaluation1['coverage']:.2f}\")\n",
    "print(f\"Avg. set size 1: {evaluation1['avg_size']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827c80b",
   "metadata": {},
   "source": [
    "### Conformal - Logit Regression (w/o protected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_calib2 = glm2.predict_proba(X_calib_s)\n",
    "\n",
    "nc_scores2 = compute_nc_scores(probs_calib2, y_calib)\n",
    "\n",
    "q_hat2 = find_threshold(nc_scores2, alpha) # q_hat is data-driven threshold for classification\n",
    "print(f\"q_hat2: {q_hat2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With test data\n",
    "pred_sets2 = predict_conformal_sets(glm2, X_test_s, q_hat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With test data\n",
    "evaluation2 = evaluate_sets(pred_sets2, y_test)\n",
    "print(f\"Coverage2: {evaluation2['coverage']:.2f}\")\n",
    "print(f\"Avg. set size 2: {evaluation2['avg_size']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244cff0",
   "metadata": {},
   "source": [
    "## Analyzing CP per group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c576df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with protected attributes\n",
    "\n",
    "# Create cp_groups with the same index as X_test_f (and y_test)\n",
    "cp_groups = pd.DataFrame(index=X_test_f.index)\n",
    "cp_groups['pred_set'] = pd.Series(pred_sets1, index=X_test_f.index).apply(lambda s: {int(x) for x in s})\n",
    "cp_groups['true_label'] = y_test.reindex(X_test_f.index)\n",
    "cp_groups['frau1'] = X_test_f['frau1']\n",
    "\n",
    "cp_groups['nongerman'] = np.where(\n",
    "    X_test_f['maxdeutsch1'] == 0, \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "cp_groups.loc[\n",
    "    X_test_f['maxdeutsch.Missing.'] == 1, \n",
    "    'nongerman'\n",
    "] = np.nan\n",
    "\n",
    "cp_groups['nongerman_male'] = np.where(\n",
    "    (cp_groups['nongerman'] == 1) & (cp_groups['frau1'] == 0),\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "cp_groups['nongerman_female'] = np.where(\n",
    "    (cp_groups['nongerman'] == 1) & (cp_groups['frau1'] == 1),\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "\n",
    "cp_groups = cp_groups.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression w/o protected attributes\n",
    "\n",
    "# Create cp_groups with the same index as X_test_s\n",
    "cp_groups2 = pd.DataFrame(index=X_test_s.index)\n",
    "\n",
    "# Assign prediction sets (assuming pred_sets2 aligns with X_test_s)\n",
    "cp_groups2['pred_set'] = pd.Series(pred_sets2, index=X_test_s.index).apply(lambda s: {int(x) for x in s})\n",
    "\n",
    "# Get true labels from y_test\n",
    "cp_groups2['true_label'] = y_test.reindex(X_test_s.index)\n",
    "\n",
    "# Bring back protected features from X_test_f (or siab_test)\n",
    "cp_groups2['frau1'] = X_test_f.loc[X_test_s.index, 'frau1']\n",
    "\n",
    "cp_groups2['nongerman'] = np.where(\n",
    "    X_test_f.loc[X_test_s.index, 'maxdeutsch1'] == 0,\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "cp_groups2.loc[\n",
    "    X_test_f.loc[X_test_s.index, 'maxdeutsch.Missing.'] == 1,\n",
    "    'nongerman'\n",
    "] = np.nan\n",
    "\n",
    "# Split by gender\n",
    "cp_groups2['nongerman_male'] = np.where(\n",
    "    (cp_groups2['nongerman'] == 1) & (cp_groups2['frau1'] == 0),\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "cp_groups2['nongerman_female'] = np.where(\n",
    "    (cp_groups2['nongerman'] == 1) & (cp_groups2['frau1'] == 1),\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "\n",
    "# Drop rows with missing data in any of the relevant columns\n",
    "cp_groups2 = cp_groups2.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f875e",
   "metadata": {},
   "source": [
    "### Conditional Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc757c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional coverage and set size\n",
    "\n",
    "# List of subgroup indicators to evaluate\n",
    "groups = ['frau1', 'nongerman', 'nongerman_male', 'nongerman_female']\n",
    "\n",
    "# Align pred_sets with y_test indices for easy filtering\n",
    "pred_sets_series = pd.Series(pred_sets2, index=y_test.index)\n",
    "\n",
    "# Prepare a list to collect results\n",
    "results = []\n",
    "\n",
    "for group in groups:\n",
    "    # Create a boolean mask for the current subgroup (True for indices in the subgroup)\n",
    "    mask = (cp_groups2[group] == 1)\n",
    "    # Align the mask to y_test index (in case cp_groups has a subset of test indices)\n",
    "    mask_aligned = mask.reindex(y_test.index, fill_value=False)\n",
    "    \n",
    "    # Filter true labels and prediction sets for this subgroup\n",
    "    group_y = y_test[mask_aligned]             # true labels for this subgroup\n",
    "    group_pred_sets = pred_sets_series[mask_aligned]  # prediction sets for this subgroup\n",
    "    \n",
    "    # Compute coverage: fraction of cases where true label is in the prediction set\n",
    "    coverage = np.mean([\n",
    "        1 if true_label in pred_set else 0 \n",
    "        for true_label, pred_set in zip(group_y, group_pred_sets)\n",
    "    ])\n",
    "    # Compute average prediction set size for this subgroup\n",
    "    avg_set_size = np.mean([len(pred_set) for pred_set in group_pred_sets])\n",
    "    \n",
    "    # Store the results (optionally multiply coverage by 100 if you want percentage)\n",
    "    results.append({\n",
    "        'Group': group,\n",
    "        'Coverage': coverage,\n",
    "        'Avg Set Size': avg_set_size,\n",
    "        'Num Samples': mask_aligned.sum()  # number of test samples in this subgroup\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for clear tabular display of the results\n",
    "coverage_results = pd.DataFrame(results).set_index('Group')\n",
    "print(coverage_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155dd8f",
   "metadata": {},
   "source": [
    "### True Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgroups true class label distributions\n",
    "\n",
    "# Overall distribution of true_label\n",
    "print(\"Overall true_label distribution:\")\n",
    "print(cp_groups2['true_label'].value_counts().sort_index())\n",
    "print(\"As proportions:\")\n",
    "print(cp_groups2['true_label'].value_counts(normalize=True).sort_index())\n",
    "print(f\"P(true_label=1): {cp_groups2['true_label'].mean():.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution conditional on frau1\n",
    "print(\"Distribution conditional on frau1:\")\n",
    "for frau_val in [0, 1]:\n",
    "    subset = cp_groups2[cp_groups2['frau1'] == frau_val] # Get all females\n",
    "    prop_positive = subset['true_label'].mean() # What % of females have true_label=1?\n",
    "    print(f\"P(true_label=1 | frau1={frau_val}): {prop_positive:.4f} (n={len(subset)})\")\n",
    "print()\n",
    "\n",
    "# Add total counts\n",
    "n_female = (cp_groups2['frau1'] == 1).sum()\n",
    "n_male = (cp_groups2['frau1'] == 0).sum()\n",
    "print(f\"Total observations: {len(cp_groups2)} (female: n={n_female}, male: n={n_male})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce790ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution conditional on nongerman\n",
    "print(\"Distribution conditional on nongerman:\")\n",
    "for ng_val in [0, 1]:\n",
    "    subset = cp_groups[cp_groups['nongerman'] == ng_val]\n",
    "    prop_positive = subset['true_label'].mean()\n",
    "    print(f\"P(true_label=1 | nongerman={ng_val}): {prop_positive:.4f} (n={len(subset)})\")\n",
    "print()\n",
    "\n",
    "# Add total counts\n",
    "n_german = (cp_groups['nongerman'] == 0).sum()\n",
    "n_nongerman = (cp_groups['nongerman'] == 1).sum()\n",
    "print(f\"Total observations: {len(cp_groups)} (german: n={n_german}, nongerman: n={n_nongerman})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution conditional on nongerman_male and nongerman_female\n",
    "print(\"Distribution conditional on nongerman subgroups:\")\n",
    "if 'nongerman_male' in cp_groups.columns:\n",
    "    for nm_val in [0, 1]:\n",
    "        subset = cp_groups[cp_groups['nongerman_male'] == nm_val]\n",
    "        prop_positive = subset['true_label'].mean()\n",
    "        print(f\"P(true_label=1 | nongerman_male={nm_val}): {prop_positive:.4f} (n={len(subset)})\")\n",
    "\n",
    "if 'nongerman_female' in cp_groups.columns:\n",
    "    for nf_val in [0, 1]:\n",
    "        subset = cp_groups[cp_groups['nongerman_female'] == nf_val]\n",
    "        prop_positive = subset['true_label'].mean()\n",
    "        print(f\"P(true_label=1 | nongerman_female={nf_val}): {prop_positive:.4f} (n={len(subset)})\")\n",
    "print()\n",
    "\n",
    "# Add total counts\n",
    "n_german_male = (cp_groups['nongerman_male'] == 0).sum()\n",
    "n_nongerman_male = (cp_groups['nongerman_male'] == 1).sum()\n",
    "print(f\"Total observations: {len(cp_groups)} (other: n={n_german_male}, nongerman male: n={n_nongerman_male})\")\n",
    "print()\n",
    "\n",
    "# Add total counts\n",
    "n_german_female = (cp_groups['nongerman_female'] == 0).sum()\n",
    "n_nongerman_female = (cp_groups['nongerman_female'] == 1).sum()\n",
    "print(f\"Total observations: {len(cp_groups)} (other: n={n_german_female}, nongerman female: n={n_nongerman_female})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d07f32",
   "metadata": {},
   "source": [
    "### Prediction Sets Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf20f9f",
   "metadata": {},
   "source": [
    "#### Summarize for Predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_for_predicate(\n",
    "    cp_groups2,\n",
    "    predicate=lambda s: set(s) == {0},\n",
    "    description=\"== {0}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513af460",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_for_predicate(\n",
    "    cp_groups2,\n",
    "    predicate=lambda s: set(s) == {1},\n",
    "    description=\"== {1}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f322fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_for_predicate(\n",
    "    cp_groups2,\n",
    "    predicate=lambda s: set(s) == {0,1},\n",
    "    description=\"== {0,1}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed81a7",
   "metadata": {},
   "source": [
    "#### Summarize by Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines CP\n",
    "\n",
    "print(\"Value counts:\")\n",
    "print(cp_groups2['pred_set'].value_counts())\n",
    "print(\"\\nProportions:\")\n",
    "print(cp_groups2['pred_set'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4da4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize for frau1 == 1 (vs 0)\n",
    "counts_female, pct_female = summarize_by_indicator(\n",
    "    cp_groups2,\n",
    "    indicator_col='frau1',\n",
    "    positive_label='female',\n",
    "    negative_label='male'\n",
    ")\n",
    "\n",
    "print(\"\\nCounts by gender:\\n\")\n",
    "print(counts_female)\n",
    "print(\"\\nPercentages by gender:\\n\")\n",
    "print(pct_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize for nongerman == 1 (vs 0)\n",
    "counts_ng, pct_ng = summarize_by_indicator(\n",
    "    cp_groups2,\n",
    "    indicator_col='nongerman',\n",
    "    positive_label='non‐German',\n",
    "    negative_label='German'\n",
    ")\n",
    "\n",
    "print(\"Counts by nationality (German vs non‐German):\\n\")\n",
    "print(counts_ng)\n",
    "print(\"\\nPercentages by nationality:\\n\")\n",
    "print(pct_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize for nongerman_male == 1 (vs 0)\n",
    "counts_ng_male, pct_ng_male = summarize_by_indicator(\n",
    "    cp_groups2,\n",
    "    indicator_col='nongerman_male',\n",
    "    positive_label='non‐German Male',\n",
    "    negative_label='Others'\n",
    ")\n",
    "\n",
    "print(\"\\nCounts for non‐German Male vs Others:\\n\")\n",
    "print(counts_ng_male)\n",
    "print(\"\\nPercentages for non‐German Male vs Others:\\n\")\n",
    "print(pct_ng_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize for nongerman_female == 1 (vs 0)\n",
    "counts_ng_female, pct_ng_female = summarize_by_indicator(\n",
    "    cp_groups2,\n",
    "    indicator_col='nongerman_female',\n",
    "    positive_label='non‐German Female',\n",
    "    negative_label='Others'\n",
    ")\n",
    "\n",
    "print(\"\\nCounts for non‐German Female vs Others:\\n\")\n",
    "print(counts_ng_female)\n",
    "print(\"\\nPercentages for non‐German Female vs Others:\\n\")\n",
    "print(pct_ng_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7553ed",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbe613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 1. Filter out ambiguous prediction sets (where pred_set == {0,1})\n",
    "confident_indices = [idx for idx, pset in enumerate(pred_sets2) if pset != {0, 1}]\n",
    "\n",
    "# If there are no confident predictions, handle that case\n",
    "if len(confident_indices) == 0:\n",
    "    print(\"No confident predictions (all predictions were ambiguous). Confusion matrix cannot be computed.\")\n",
    "else:\n",
    "    # 2. Extract predicted labels from the remaining sets\n",
    "    y_pred_filtered = []\n",
    "    for idx in confident_indices:\n",
    "        pset = pred_sets2[idx]\n",
    "        # pset can only be {0} or {1} here\n",
    "        predicted_label = 0 if pset == {0} else 1\n",
    "        y_pred_filtered.append(predicted_label)\n",
    "\n",
    "    # 3. Align predicted labels with the corresponding true labels\n",
    "    # Use the same indices to filter y_test\n",
    "    y_true_filtered = [y_test.iloc[idx] for idx in confident_indices]\n",
    "\n",
    "    # 4. Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true_filtered, y_pred_filtered)\n",
    "    print(\"Confusion matrix (excluding ambiguous cases):\")\n",
    "    print(cm)\n",
    "\n",
    "# Extract individual components\n",
    "TN, FP, FN, TP = cm.ravel()  # Unpacks the 2x2 matrix into values\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else float('nan')\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else float('nan')\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else float('nan')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1 Score:  {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009dc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = cp_groups2.index\n",
    "# Align arrays\n",
    "pred_sets_filtered = [pred_sets2[i] for i in valid_idx]\n",
    "y_test_filtered = np.array(y_test)[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693daaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_confusion_metrics(pred_sets, y_true, subgroup_mask):\n",
    "    # Filter to non-ambiguous predictions and apply subgroup mask\n",
    "    mask = np.array([len(s) == 1 for s in pred_sets]) & subgroup_mask\n",
    "    if not np.any(mask):\n",
    "        return None  # no data to evaluate\n",
    "    \n",
    "    y_true_filtered = np.array(y_true)[mask]\n",
    "    y_pred_filtered = [list(s)[0] for i, s in enumerate(pred_sets) if len(s) == 1 and subgroup_mask[i]]\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true_filtered, y_pred_filtered, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"Coverage (non-ambiguous)\": np.mean(mask)\n",
    "    }\n",
    "\n",
    "#frau1_mask = cp_groups['frau1'] == 1\n",
    "#nongerman_mask = cp_groups['nongerman'] == 1\n",
    "#nongerman_male_mask = cp_groups['nongerman_male'] == 1\n",
    "#nongerman_female_mask = cp_groups['nongerman_female'] == 1\n",
    "#\n",
    "## Create a dictionary of subgroups\n",
    "#subgroups = {\n",
    "#    \"frau1\": frau1_mask,\n",
    "#    \"nongerman\": nongerman_mask,\n",
    "#    \"nongerman_male\": nongerman_male_mask,\n",
    "#    \"nongerman_female\": nongerman_female_mask\n",
    "#}\n",
    "\n",
    "subgroups = {\n",
    "    \"frau1\": (cp_groups2[\"frau1\"] == 1).values,\n",
    "    \"nongerman\": (cp_groups2[\"nongerman\"] == 1).values,\n",
    "    \"nongerman_male\": (cp_groups2[\"nongerman_male\"] == 1).values,\n",
    "    \"nongerman_female\": (cp_groups2[\"nongerman_female\"] == 1).values\n",
    "}\n",
    "\n",
    "# Example usage:\n",
    "results = {}\n",
    "for name, mask in subgroups.items():\n",
    "    metrics = compute_confusion_metrics(pred_sets_filtered, y_test_filtered, mask)\n",
    "    if metrics:\n",
    "        results[name] = metrics\n",
    "\n",
    "# Print nicely\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results.index.name = \"Subgroup\"\n",
    "display(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f2ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
