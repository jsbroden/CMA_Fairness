{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e606c9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, make_scorer, roc_curve, auc, precision_recall_curve, classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score, log_loss, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc99775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import aif_test, aif_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f = pd.read_csv(\"./output/X_train_f.csv\")\n",
    "X_train_s = pd.read_csv(\"./output/X_train_s.csv\")\n",
    "\n",
    "X_test_f = pd.read_csv(\"./output/X_test_f.csv\")\n",
    "X_test_s = pd.read_csv(\"./output/X_test_s.csv\")\n",
    "y_test = pd.read_csv(\"./output/y_test.csv\")\n",
    "\n",
    "preds_test = pd.read_csv(\"./output/preds_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ebe95",
   "metadata": {},
   "source": [
    "# 00 Add rule-based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b810fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column skill_rule\n",
    "# if a person has none of the education levels (and no missing data), label them with skill_rule = 1 (likely no formal education)\n",
    "\n",
    "preds_test['skill_rule'] = np.where((X_test_s['maxausbildung_imp2'] == 0) & \n",
    "                                    (X_test_s['maxausbildung_imp3'] == 0) & \n",
    "                                    (X_test_s['maxausbildung_imp4'] == 0) & \n",
    "                                    (X_test_s['maxausbildung_imp5'] == 0) & \n",
    "                                    (X_test_s['maxausbildung_imp6'] == 0) &\n",
    "                                    (X_test_s['maxausbildung_imp.Missing.'] == 0), 1, 0)\n",
    "preds_test['skill_rule'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f61f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_rule = 1, if seeking for a job more than 2 years (730 days)\n",
    "\n",
    "preds_test['time_rule'] = np.where((X_test_s['seeking1_tot_dur'] > 730), 1, 0)\n",
    "preds_test['time_rule'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a404134",
   "metadata": {},
   "source": [
    "## 01a Overall Performance (w. protected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve, returns FPR at each threshold, TPR at each threshold\n",
    "# thresholds1 are thereshold values used to compute FPR and TPR\n",
    "fpr1, tpr1, thresholds1 = roc_curve(preds_test['y_test'], preds_test['glm1_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC (Area Under the Curve) for the ROC curve\n",
    "# interpreation: 1 perfect classifier, 0.5 no better than random guessing\n",
    "rocauc_glm1 = auc(fpr1, tpr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve, returns precision at each threshold, recall at each threshold\n",
    "# usefull for imbalanced datasets\n",
    "# care more about the positive class (1)\n",
    "prec1, rec1, thresholds1 = precision_recall_curve(preds_test['y_test'], preds_test['glm1_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d41ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under the Precision-Recall curve\n",
    "# interpreation: 1 perfect classifier, 0 no better than random guessing\n",
    "prauc_glm1 = auc(rec1, prec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfdd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curves\n",
    "random_probs = [0 for i in range(len(preds_test['y_test']))] # random predictions, no skill classifier (diagonal line)\n",
    "p_fpr, p_tpr, _ = roc_curve(preds_test['y_test'], random_probs, pos_label = 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 7))\n",
    "plt.plot(fpr1, tpr1, color = 'blue', label = 'Logistic Regression')\n",
    "#plt.plot(fpr2, tpr2, color = 'green', label = 'Elastic Net')\n",
    "#plt.plot(fpr3, tpr3, color = 'orange', label = 'Random Forest')\n",
    "#plt.plot(fpr4, tpr4, color = 'red', label = 'Gradient Boosting')\n",
    "plt.plot(p_fpr, p_tpr, linestyle = '--', color = 'black')\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "plt.xlabel('False Positive Rate', fontsize = 18)\n",
    "plt.ylabel('True Positive Rate', fontsize = 18)\n",
    "plt.legend(loc = 'best', fontsize = 16)\n",
    "plt.savefig('./output/ROC1', dpi = 300)\n",
    "plt.show();\n",
    "\n",
    "# interpretation of the ROC curve:\n",
    "# - the closer the curve is to the top left corner, the better the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5faca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR curves\n",
    "fig, ax = plt.subplots(figsize = (9, 7))\n",
    "plt.plot(rec1, prec1, color = 'blue', label = 'Logistic Regression')\n",
    "#plt.plot(rec2, prec2, color = 'green', label = 'Elastic Net')\n",
    "#plt.plot(rec3, prec3, color = 'orange', label = 'Random Forest')\n",
    "#plt.plot(rec4, prec4, color = 'red', label = 'Gradient Boosting')\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "plt.xlabel('Recall', fontsize = 18)\n",
    "plt.ylabel('Precision', fontsize = 18)\n",
    "plt.legend(loc = 'best', fontsize = 16)\n",
    "plt.savefig('./output/PR1', dpi = 300)\n",
    "plt.show();\n",
    "\n",
    "# interpretation of the PR curve:\n",
    "# - the closer the curve is to the top right corner, the better the model\n",
    "# - the area under the curve (AUC) is a measure of the model's performance\n",
    "# - the higher the AUC, the better the model\n",
    "# - the AUC is not affected by the class imbalance, unlike the ROC AUC\n",
    "\n",
    "# LR has low precision, indicates high number of false positives (-> class imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b803f",
   "metadata": {},
   "source": [
    "# 02 Classification Performance (rule-based predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68640107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes precision, recall, f1-score, accuracy for class 1 for skill rule as classifier\n",
    "srule_rep = classification_report(preds_test['y_test'], preds_test['skill_rule'], output_dict = True)\n",
    "\n",
    "# create a DataFrame with the results for skill_rule (acc, f1, prec, rec)\n",
    "# treats skill_rule as heuristic classifier and measures how well it predicts y_test\n",
    "# to compare simple rules (like skill_rule) with more complex models (like glm1)\n",
    "srule_perf = pd.DataFrame(np.array([srule_rep['accuracy'], srule_rep['1']['f1-score'], srule_rep['1']['precision'], srule_rep['1']['recall']]), columns = ['skill_rule'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "trule_rep = classification_report(preds_test['y_test'], preds_test['time_rule'], output_dict = True)\n",
    "trule_perf = pd.DataFrame(np.array([trule_rep['accuracy'], trule_rep['1']['f1-score'], trule_rep['1']['precision'], trule_rep['1']['recall']]), columns = ['time_rule'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf0 = pd.concat([srule_perf,\n",
    "                   trule_perf], \n",
    "                  axis = 1).transpose()\n",
    "\n",
    "# df with skill_rule and time_rule performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf0 = perf0.rename(columns={0: \"Accuracy\", 1: \"F1 Score\", 2: \"Precision\", 3: \"Recall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf0.to_latex('./output/test_perf0.tex', index = False, float_format = \"%.3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5bcc6e",
   "metadata": {},
   "source": [
    "# 02a Classification Performance (w. protected attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94892709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compares ture labels with predictions of glm1_c1 (top 15%)\n",
    "# returns dict with prec, rec, f1, acc for each class\n",
    "glm1_c1_rep = classification_report(preds_test['y_test'], preds_test['glm1_c1'], output_dict = True)\n",
    "\n",
    "# balanced accuracy \n",
    "glm1_c1_acc = balanced_accuracy_score(preds_test['y_test'], preds_test['glm1_c1'])\n",
    "\n",
    "# roc auc, pr auc, acc, balanced acc, f1, prec, recall for glm1_c1\n",
    "glm1_c1_perf = pd.DataFrame(np.array([rocauc_glm1, \n",
    "                                      prauc_glm1, \n",
    "                                      glm1_c1_rep['accuracy'], \n",
    "                                      glm1_c1_acc, \n",
    "                                      glm1_c1_rep['1']['f1-score'], \n",
    "                                      glm1_c1_rep['1']['precision'], \n",
    "                                      glm1_c1_rep['1']['recall']\n",
    "                                    ]), \n",
    "                                    columns = ['glm1_c1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm1_c2_rep = classification_report(preds_test['y_test'], preds_test['glm1_c2'], output_dict = True)\n",
    "glm1_c2_acc = balanced_accuracy_score(preds_test['y_test'], preds_test['glm1_c2'])\n",
    "glm1_c2_perf = pd.DataFrame(np.array([rocauc_glm1, prauc_glm1, glm1_c2_rep['accuracy'], glm1_c2_acc, glm1_c2_rep['1']['f1-score'], glm1_c2_rep['1']['precision'], glm1_c2_rep['1']['recall']]), columns = ['glm1_c2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da60ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf1_1 = pd.concat([glm1_c1_perf,\n",
    "                     #net1_c1_perf,\n",
    "                     #rf1_c1_perf,\n",
    "                     #gbm1_c1_perf\n",
    "                     ], \n",
    "                    axis = 1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf1_1 = perf1_1.rename(columns={0: \"ROC-AUC\", 1: \"PR-AUC\", 2: \"Accuracy\", 3: \"Balanced Accuracy\", 4: \"F1 Score\", 5: \"Precision\", 6: \"Recall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf1_1.to_latex('./output/test_perf1_c1.tex', index = False, float_format = \"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf1_2 = pd.concat([glm1_c2_perf,\n",
    "                     #net1_c2_perf,\n",
    "                     #rf1_c2_perf,\n",
    "                     #gbm1_c2_perf\n",
    "                     ], \n",
    "                    axis = 1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf1_2 = perf1_2.rename(columns={0: \"ROC-AUC\", 1: \"PR-AUC\", 2: \"Accuracy\", 3: \"Balanced Accuracy\", 4: \"F1 Score\", 5: \"Precision\", 6: \"Recall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf1_2.to_latex('./output/test_perf1_c2.tex', index = False, float_format = \"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add performance evaluation of glm1_c3 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf005d",
   "metadata": {},
   "source": [
    "# 03 Classification Performance by Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23498e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test = pd.concat([preds_test, X_test_f], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36569223",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test['nongerman'] = np.where(comb_test['maxdeutsch1'] == 0, 1, 0)\n",
    "comb_test.loc[comb_test['maxdeutsch.Missing.'] == 1, 'nongerman'] = np.nan\n",
    "comb_test['nongerman_male'] = np.where((comb_test['nongerman'] == 1) & (comb_test['frau1'] == 0), 1, 0)\n",
    "comb_test['nongerman_female'] = np.where((comb_test['nongerman'] == 1) & (comb_test['frau1'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test = comb_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = []\n",
    "pops = ('frau1', 'nongerman', 'nongerman_male', 'nongerman_female') # population subgroups\n",
    "\n",
    "# compute f1 score for overall population \n",
    "f1.append(['Overall',\n",
    "           f1_score(comb_test['y_test'], comb_test['glm1_c1']), \n",
    "           f1_score(comb_test['y_test'], comb_test['glm1_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['glm2_c1']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['glm2_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['glm1b_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['glm1b_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['glm2b_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['glm2b_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['net1_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['net1_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['net2_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['net2_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['net1b_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['net1b_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['net2b_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['net2b_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['rf1_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['rf1_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['rf2_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['rf2_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['rf1b_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['rf1b_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['rf2b_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['rf2b_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['gbm1_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['gbm1_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['gbm2_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['gbm2_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['gbm1b_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['gbm1b_c2']),\n",
    "           #f1_score(comb_test['y_test'], comb_test['gbm2b_c1']), \n",
    "           #f1_score(comb_test['y_test'], comb_test['gbm2b_c2'])\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops over subgroups and computes f1 scores for each model variant\n",
    "\n",
    "for pop in pops: \n",
    "    subset = (comb_test[pop] == 1)\n",
    "    f1.append([pop,\n",
    "                f1_score(comb_test[subset]['y_test'], comb_test[subset]['glm1_c1']),\n",
    "                f1_score(comb_test[subset]['y_test'], comb_test[subset]['glm1_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['glm2_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['glm2_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['glm1b_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['glm1b_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['glm2b_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['glm2b_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['net1_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['net1_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['net2_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['net2_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['net1b_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['net1b_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['net2b_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['net2b_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['rf1_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['rf1_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['rf2_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['rf2_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['rf1b_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['rf1b_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['rf2b_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['rf2b_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['gbm1_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['gbm1_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['gbm2_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['gbm2_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['gbm1b_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['gbm1b_c2']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['gbm2b_c1']),\n",
    "                #f1_score(comb_test[subset]['y_test'], comb_test[subset]['gbm2b_c2'])\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_group = pd.DataFrame(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_group = f1_group.rename(columns={0: 'pop', \n",
    "                                    1: 'LR_1_c1', 2: 'LR_1_c2', 3: 'LR_2_c1', 4: 'LR_2_c2', \n",
    "                                    5: 'LR_1b_c1', 6: 'LR_1b_c2', 7: 'LR_2b_c1', 8: 'LR_2b_c2',\n",
    "                                    9: 'PLR_1_c1', 10: 'PLR_1_c2', 11: 'PLR_2_c1', 12: 'PLR_2_c2', \n",
    "                                    13: 'PLR_1b_c1', 14: 'PLR_1b_c2', 15: 'PLR_2b_c1', 16: 'PLR_2b_c2',\n",
    "                                    17: 'RF_1_c1', 18: 'RF_1_c2', 19: 'RF_2_c1', 20: 'RF_2_c2', \n",
    "                                    21: 'RF_1b_c1', 22: 'RF_1b_c2', 23: 'RF_2b_c1', 24: 'RF_2b_c2', \n",
    "                                    25: 'GBM_1_c1', 26: 'GBM_1_c2', 27: 'GBM_2_c1', 28: 'GBM_2_c2',\n",
    "                                    29: 'GBM_1b_c1', 30: 'GBM_1b_c2', 31: 'GBM_2b_c1', 32: 'GBM_2b_c2'})\n",
    "f1_group['pop'] = ['Overall', 'Female', 'Non-German', 'Non-Ger. M', 'Non-Ger. F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms wide-format f1 score table into long-format\n",
    "# value contains f1-score for given combiantion (subgroup (pop), model, cutoff)\n",
    "# type contains ???\n",
    "\n",
    "f1_group_l1 = pd.wide_to_long(f1_group, stubnames=['LR', 'PLR', 'RF', 'GBM'], i=['pop'], j='model', sep='_', suffix='\\w+')\n",
    "f1_group_l1 = f1_group_l1.reset_index()\n",
    "f1_group_l2 = f1_group_l1.melt(id_vars=['pop', 'model'], var_name='method')\n",
    "f1_group_l2[['Type', 'Cutoff']] = f1_group_l2['model'].str.split(\"_\", expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_group_l2.to_csv('./output/f1_group.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5423c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced accuracy for each model variant\n",
    "# overall means evaluated on the whole dataset\n",
    "\n",
    "acc = []\n",
    "\n",
    "acc.append(['Overall',\n",
    "            balanced_accuracy_score(comb_test['y_test'], comb_test['glm1_c1']),\n",
    "            balanced_accuracy_score(comb_test['y_test'], comb_test['glm1_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['glm2_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['glm2_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['glm1b_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['glm1b_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['glm2b_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['glm2b_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['net1_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['net1_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['net2_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['net2_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['net1b_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['net1b_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['net2b_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['net2b_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['rf1_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['rf1_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['rf2_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['rf2_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['rf1b_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['rf1b_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['rf2b_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['rf2b_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['gbm1_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['gbm1_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['gbm2_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['gbm2_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['gbm1b_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['gbm1b_c2']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['gbm2b_c1']),\n",
    "            #balanced_accuracy_score(comb_test['y_test'], comb_test['gbm2b_c2'])\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pop in pops: \n",
    "    subset = (comb_test[pop] == 1)\n",
    "    acc.append([pop,\n",
    "                balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['glm1_c1']),\n",
    "                balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['glm1_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['glm2_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['glm2_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['glm1b_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['glm1b_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['glm2b_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['glm2b_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['net1_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['net1_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['net2_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['net2_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['net1b_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['net1b_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['net2b_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['net2b_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['rf1_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['rf1_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['rf2_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['rf2_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['rf1b_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['rf1b_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['rf2b_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['rf2b_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['gbm1_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['gbm1_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['gbm2_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['gbm2_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['gbm1b_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['gbm1b_c2']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['gbm2b_c1']),\n",
    "                #balanced_accuracy_score(comb_test[subset]['y_test'], comb_test[subset]['gbm2b_c2'])\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb26574",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_group = pd.DataFrame(acc)\n",
    "\n",
    "acc_group = acc_group.rename(columns={0: 'pop', \n",
    "                                      1: 'LR_1_c1', 2: 'LR_1_c2', 3: 'LR_2_c1', 4: 'LR_2_c2', \n",
    "                                      5: 'LR_1b_c1', 6: 'LR_1b_c2', 7: 'LR_2b_c1', 8: 'LR_2b_c2',\n",
    "                                      9: 'PLR_1_c1', 10: 'PLR_1_c2', 11: 'PLR_2_c1', 12: 'PLR_2_c2', \n",
    "                                      13: 'PLR_1b_c1', 14: 'PLR_1b_c2', 15: 'PLR_2b_c1', 16: 'PLR_2b_c2',\n",
    "                                      17: 'RF_1_c1', 18: 'RF_1_c2', 19: 'RF_2_c1', 20: 'RF_2_c2', \n",
    "                                      21: 'RF_1b_c1', 22: 'RF_1b_c2', 23: 'RF_2b_c1', 24: 'RF_2b_c2', \n",
    "                                      25: 'GBM_1_c1', 26: 'GBM_1_c2', 27: 'GBM_2_c1', 28: 'GBM_2_c2',\n",
    "                                      29: 'GBM_1b_c1', 30: 'GBM_1b_c2', 31: 'GBM_2b_c1', 32: 'GBM_2b_c2'})\n",
    "acc_group['pop'] = ['Overall', 'Female', 'Non-German', 'Non-Ger. M', 'Non-Ger. F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65291e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_group_l1 = pd.wide_to_long(acc_group, stubnames=['LR', 'PLR', 'RF', 'GBM'], i=['pop'], j='model', sep='_', suffix='\\w+')\n",
    "acc_group_l1 = acc_group_l1.reset_index()\n",
    "acc_group_l2 = acc_group_l1.melt(id_vars=['pop', 'model'], var_name='method')\n",
    "acc_group_l2[['Type', 'Cutoff']] = acc_group_l2['model'].str.split(\"_\", expand = True)\n",
    "\n",
    "acc_group_l2.to_csv('./output/acc_group.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d135d5",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795aa8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize f1 scores for each subgroup\n",
    "\n",
    "sns.stripplot(x = \"value\", y = \"pop\", data = f1_group_l2, alpha = .5, zorder = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x = \"value\", y = \"pop\", hue = \"method\", dodge = True, data = f1_group_l2, zorder = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1642a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x = \"value\", y = \"pop\", hue = \"Cutoff\", dodge = True, data = f1_group_l2, zorder = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5905b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1-score distr. for subgroups and different model cutoffs/thresholds\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize = (8.5, 7))\n",
    "ax1 = sns.boxplot(x = \"value\", y = \"pop\", hue = \"Cutoff\", dodge = True, data = f1_group_l2, linewidth = 1.25, fliersize = 0)\n",
    "ax1.xaxis.set_tick_params(labelsize = 15)\n",
    "ax1.yaxis.set_tick_params(labelsize = 15)\n",
    "ax2 = sns.stripplot(x = \"value\", y = \"pop\", hue = \"Cutoff\", dodge = True, data = f1_group_l2, palette = \"muted\")\n",
    "ax2.set_xlabel(\"Performance Score\", fontsize = 15)\n",
    "ax2.set_ylabel(\"\")\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "plt.legend(handles[2:4], ['Policy 1a', 'Policy 1b'], bbox_to_anchor = (0.975, 0.55), loc = 2, fontsize = 14)\n",
    "plt.setp(ax1.artists, fill = False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/group_f1', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize balanced accuracy scores for each subgroup\n",
    "\n",
    "sns.stripplot(x = \"value\", y = \"pop\", data = acc_group_l2, alpha = .5, zorder = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957117c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x = \"value\", y = \"pop\", hue = \"method\", dodge = True, data = acc_group_l2, zorder = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x = \"value\", y = \"pop\", hue = \"Cutoff\", dodge = True, data = acc_group_l2, zorder = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce309d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced acc. distr. for subgroups and different model cutoffs/thresholds\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize = (8.5, 7))\n",
    "ax1 = sns.boxplot(x = \"value\", y = \"pop\", hue = \"Cutoff\", dodge = True, data = acc_group_l2, linewidth = 1.25, fliersize = 0)\n",
    "ax1.xaxis.set_tick_params(labelsize = 15)\n",
    "ax1.yaxis.set_tick_params(labelsize = 15)\n",
    "ax2 = sns.stripplot(x = \"value\", y = \"pop\", hue = \"Cutoff\", dodge = True, data = acc_group_l2, palette = \"muted\")\n",
    "ax2.set_xlabel(\"Performance Score\", fontsize = 15)\n",
    "ax2.set_ylabel(\"\")\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "plt.legend(handles[2:4], ['Policy 1a', 'Policy 1b'], bbox_to_anchor = (0.975, 0.55), loc = 2, fontsize = 14)\n",
    "plt.setp(ax1.artists, fill = False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/group_acc', dpi = 300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
